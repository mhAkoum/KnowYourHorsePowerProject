{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of AI_project_Main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8nNK0a-hQUM"
      },
      "source": [
        "import of lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxSxOqE_H0CW"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_uWwjSF3_Qo"
      },
      "source": [
        "import for the data file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AokVaF1CzUvl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "9e65a977-53a1-4cbd-d7ec-f530efa144f4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/CarPrice_Assignment.csv\")\n",
        "data.head()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>car_ID</th>\n",
              "      <th>symboling</th>\n",
              "      <th>CarName</th>\n",
              "      <th>fueltype</th>\n",
              "      <th>aspiration</th>\n",
              "      <th>doornumber</th>\n",
              "      <th>carbody</th>\n",
              "      <th>drivewheel</th>\n",
              "      <th>enginelocation</th>\n",
              "      <th>wheelbase</th>\n",
              "      <th>carlength</th>\n",
              "      <th>carwidth</th>\n",
              "      <th>carheight</th>\n",
              "      <th>curbweight</th>\n",
              "      <th>enginetype</th>\n",
              "      <th>cylindernumber</th>\n",
              "      <th>enginesize</th>\n",
              "      <th>fuelsystem</th>\n",
              "      <th>boreratio</th>\n",
              "      <th>stroke</th>\n",
              "      <th>compressionratio</th>\n",
              "      <th>peakrpm</th>\n",
              "      <th>citympg</th>\n",
              "      <th>highwaympg</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>alfa-romero giulia</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>two</td>\n",
              "      <td>convertible</td>\n",
              "      <td>rwd</td>\n",
              "      <td>front</td>\n",
              "      <td>88.6</td>\n",
              "      <td>168.8</td>\n",
              "      <td>64.1</td>\n",
              "      <td>48.8</td>\n",
              "      <td>2548</td>\n",
              "      <td>dohc</td>\n",
              "      <td>four</td>\n",
              "      <td>130</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>3.47</td>\n",
              "      <td>2.68</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5000</td>\n",
              "      <td>21</td>\n",
              "      <td>27</td>\n",
              "      <td>111</td>\n",
              "      <td>13495.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>alfa-romero stelvio</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>two</td>\n",
              "      <td>convertible</td>\n",
              "      <td>rwd</td>\n",
              "      <td>front</td>\n",
              "      <td>88.6</td>\n",
              "      <td>168.8</td>\n",
              "      <td>64.1</td>\n",
              "      <td>48.8</td>\n",
              "      <td>2548</td>\n",
              "      <td>dohc</td>\n",
              "      <td>four</td>\n",
              "      <td>130</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>3.47</td>\n",
              "      <td>2.68</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5000</td>\n",
              "      <td>21</td>\n",
              "      <td>27</td>\n",
              "      <td>111</td>\n",
              "      <td>16500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>alfa-romero Quadrifoglio</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>two</td>\n",
              "      <td>hatchback</td>\n",
              "      <td>rwd</td>\n",
              "      <td>front</td>\n",
              "      <td>94.5</td>\n",
              "      <td>171.2</td>\n",
              "      <td>65.5</td>\n",
              "      <td>52.4</td>\n",
              "      <td>2823</td>\n",
              "      <td>ohcv</td>\n",
              "      <td>six</td>\n",
              "      <td>152</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>2.68</td>\n",
              "      <td>3.47</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5000</td>\n",
              "      <td>19</td>\n",
              "      <td>26</td>\n",
              "      <td>154</td>\n",
              "      <td>16500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>audi 100 ls</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>four</td>\n",
              "      <td>sedan</td>\n",
              "      <td>fwd</td>\n",
              "      <td>front</td>\n",
              "      <td>99.8</td>\n",
              "      <td>176.6</td>\n",
              "      <td>66.2</td>\n",
              "      <td>54.3</td>\n",
              "      <td>2337</td>\n",
              "      <td>ohc</td>\n",
              "      <td>four</td>\n",
              "      <td>109</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>3.19</td>\n",
              "      <td>3.40</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5500</td>\n",
              "      <td>24</td>\n",
              "      <td>30</td>\n",
              "      <td>102</td>\n",
              "      <td>13950.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>audi 100ls</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>four</td>\n",
              "      <td>sedan</td>\n",
              "      <td>4wd</td>\n",
              "      <td>front</td>\n",
              "      <td>99.4</td>\n",
              "      <td>176.6</td>\n",
              "      <td>66.4</td>\n",
              "      <td>54.3</td>\n",
              "      <td>2824</td>\n",
              "      <td>ohc</td>\n",
              "      <td>five</td>\n",
              "      <td>136</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>3.19</td>\n",
              "      <td>3.40</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5500</td>\n",
              "      <td>18</td>\n",
              "      <td>22</td>\n",
              "      <td>115</td>\n",
              "      <td>17450.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   car_ID  symboling                   CarName  ... highwaympg horsepower    price\n",
              "0       1          3        alfa-romero giulia  ...         27        111  13495.0\n",
              "1       2          3       alfa-romero stelvio  ...         27        111  16500.0\n",
              "2       3          1  alfa-romero Quadrifoglio  ...         26        154  16500.0\n",
              "3       4          2               audi 100 ls  ...         30        102  13950.0\n",
              "4       5          2                audi 100ls  ...         22        115  17450.0\n",
              "\n",
              "[5 rows x 26 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DylDd8f9V3p",
        "outputId": "cf7430e5-3b33-4c70-8f44-dc3a25583728"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 205 entries, 0 to 204\n",
            "Data columns (total 26 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   car_ID            205 non-null    int64  \n",
            " 1   symboling         205 non-null    int64  \n",
            " 2   CarName           205 non-null    object \n",
            " 3   fueltype          205 non-null    object \n",
            " 4   aspiration        205 non-null    object \n",
            " 5   doornumber        205 non-null    object \n",
            " 6   carbody           205 non-null    object \n",
            " 7   drivewheel        205 non-null    object \n",
            " 8   enginelocation    205 non-null    object \n",
            " 9   wheelbase         205 non-null    float64\n",
            " 10  carlength         205 non-null    float64\n",
            " 11  carwidth          205 non-null    float64\n",
            " 12  carheight         205 non-null    float64\n",
            " 13  curbweight        205 non-null    int64  \n",
            " 14  enginetype        205 non-null    object \n",
            " 15  cylindernumber    205 non-null    object \n",
            " 16  enginesize        205 non-null    int64  \n",
            " 17  fuelsystem        205 non-null    object \n",
            " 18  boreratio         205 non-null    float64\n",
            " 19  stroke            205 non-null    float64\n",
            " 20  compressionratio  205 non-null    float64\n",
            " 21  peakrpm           205 non-null    int64  \n",
            " 22  citympg           205 non-null    int64  \n",
            " 23  highwaympg        205 non-null    int64  \n",
            " 24  horsepower        205 non-null    int64  \n",
            " 25  price             205 non-null    float64\n",
            "dtypes: float64(8), int64(8), object(10)\n",
            "memory usage: 41.8+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fAX3GJM4Mnv"
      },
      "source": [
        "# drop out the unecerri data \n",
        "drop out the columns of data that do not effect the horspower"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "059WO9GJLETO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3edcecee-b51f-4583-c0fc-1575dcc2e4f9"
      },
      "source": [
        "data.drop('car_ID',axis = 1,inplace = True)\n",
        "data.drop('CarName',axis = 1,inplace = True)\n",
        "data.drop('price',axis = 1,inplace = True)\n",
        "data.drop('doornumber',axis = 1,inplace = True)\n",
        "data.drop('carbody',axis = 1,inplace = True)\n",
        "data.drop('citympg',axis = 1,inplace = True)\n",
        "data.drop('highwaympg',axis = 1,inplace = True)\n",
        "data.drop('symboling',axis = 1,inplace = True)\n",
        "\n",
        "data.info()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 205 entries, 0 to 204\n",
            "Data columns (total 18 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   fueltype          205 non-null    object \n",
            " 1   aspiration        205 non-null    object \n",
            " 2   drivewheel        205 non-null    object \n",
            " 3   enginelocation    205 non-null    object \n",
            " 4   wheelbase         205 non-null    float64\n",
            " 5   carlength         205 non-null    float64\n",
            " 6   carwidth          205 non-null    float64\n",
            " 7   carheight         205 non-null    float64\n",
            " 8   curbweight        205 non-null    int64  \n",
            " 9   enginetype        205 non-null    object \n",
            " 10  cylindernumber    205 non-null    object \n",
            " 11  enginesize        205 non-null    int64  \n",
            " 12  fuelsystem        205 non-null    object \n",
            " 13  boreratio         205 non-null    float64\n",
            " 14  stroke            205 non-null    float64\n",
            " 15  compressionratio  205 non-null    float64\n",
            " 16  peakrpm           205 non-null    int64  \n",
            " 17  horsepower        205 non-null    int64  \n",
            "dtypes: float64(7), int64(4), object(7)\n",
            "memory usage: 29.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CytnShkf4Rhr"
      },
      "source": [
        "# change of data type\n",
        "data parameters have deferent types. Data of type object must be changed to float64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6KGlNB61jH1"
      },
      "source": [
        "unsing data replacers\n",
        "we changed the object data with only 2 different objects(\"strings\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syPLz_rCLHLX"
      },
      "source": [
        "def change_name_ft(fueltype):\n",
        "  if fueltype == \"gas\":\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "def change_name_asp(aspiration):\n",
        "  if aspiration == \"std\":\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "def change_name_enginelocation(enginelocation):\n",
        "  if enginelocation =='front':\n",
        "    return 0\n",
        "  else:\n",
        "    return 1  "
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMtg4iPAHb8k"
      },
      "source": [
        "imported method to transfrom the cylinders column from letters to an int using text2int method (from stackoverflow) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRBWjrv7GRor"
      },
      "source": [
        "def text2int (textnum, numwords={}):\n",
        "    if not numwords:\n",
        "        units = [\n",
        "        \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
        "        \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
        "        \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\",\n",
        "        ]\n",
        "\n",
        "        #tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]\n",
        "        #scales = [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]\n",
        "\n",
        "        numwords[\"and\"] = (1, 0)\n",
        "        for idx, word in enumerate(units):  numwords[word] = (1, idx)\n",
        "        #for idx, word in enumerate(tens):       numwords[word] = (1, idx * 10)\n",
        "        #for idx, word in enumerate(scales): numwords[word] = (10 ** (idx * 3 or 2), 0)\n",
        "\n",
        "    ordinal_words = {'first':1, 'second':2, 'third':3, 'fifth':5, 'eighth':8, 'ninth':9, 'twelfth':12}\n",
        "    ordinal_endings = [('ieth', 'y'), ('th', '')]\n",
        "\n",
        "    textnum = textnum.replace('-', ' ')\n",
        "\n",
        "    current = result = 0\n",
        "    curstring = \"\"\n",
        "    onnumber = False\n",
        "    for word in textnum.split():\n",
        "        if word in ordinal_words:\n",
        "            scale, increment = (1, ordinal_words[word])\n",
        "            current = current * scale + increment\n",
        "            if scale > 100:\n",
        "                result += current\n",
        "                current = 0\n",
        "            onnumber = True\n",
        "        else:\n",
        "            for ending, replacement in ordinal_endings:\n",
        "                if word.endswith(ending):\n",
        "                    word = \"%s%s\" % (word[:-len(ending)], replacement)\n",
        "\n",
        "            if word not in numwords:\n",
        "                if onnumber:\n",
        "                    curstring += repr(result + current) + \" \"\n",
        "                curstring += word + \" \"\n",
        "                result = current = 0\n",
        "                onnumber = False\n",
        "            else:\n",
        "                scale, increment = numwords[word]\n",
        "\n",
        "                current = current * scale + increment\n",
        "                if scale > 100:\n",
        "                    result += current\n",
        "                    current = 0\n",
        "                onnumber = True\n",
        "\n",
        "    if onnumber:\n",
        "        curstring += repr(result + current)\n",
        "\n",
        "    return int(curstring)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI1KEyDIH1Cz"
      },
      "source": [
        "# impliment one hot incoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWEITAYwbZXI"
      },
      "source": [
        "incoder1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmCjIA_s2wi9"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "dataset = data.values\n",
        "x1 = dataset[:,2]\n",
        "encoder1 = LabelEncoder()\n",
        "encoder1.fit(x1)\n",
        "encoded_x1 = encoder1.transform(x1)\n",
        "dummy_x1 = to_categorical(encoded_x1)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LsKUOjcba5M"
      },
      "source": [
        "incoder2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShR90vJfH0g_"
      },
      "source": [
        "dataset = data.values\n",
        "x2 = dataset[:,9]\n",
        "encoder2 = LabelEncoder()\n",
        "encoder2.fit(x2)\n",
        "encoded_x2 = encoder2.transform(x2)\n",
        "dummy_x2 = to_categorical(encoded_x2)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ_VvUarbly6"
      },
      "source": [
        "incoder3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gemuD8-HbctT"
      },
      "source": [
        "dataset = data.values\n",
        "x3 = dataset[:,12] \n",
        "encoder3 = LabelEncoder()\n",
        "encoder3.fit(x3)\n",
        "encoded_x3 = encoder3.transform(x3)\n",
        "dummy_x3 = to_categorical(encoded_x3)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R_KKgXTbqpw"
      },
      "source": [
        "apply of the transformation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2e98KtV-XPv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "9a7e80bd-e842-436d-a474-c13dde4cf27d"
      },
      "source": [
        "data[\"fueltype\"] = data[\"fueltype\"].apply(lambda x : change_name_ft(x))\n",
        "data[\"aspiration\"] = data[\"aspiration\"].apply(lambda x : change_name_asp(x))\n",
        "data[\"enginelocation\"] = data[\"enginelocation\"].apply(lambda x : change_name_enginelocation(x))\n",
        "data['drivewheel'] = encoder1.fit_transform(encoded_x1)\n",
        "data['enginetype'] = encoder2.fit_transform(encoded_x2) \n",
        "data['cylindernumber'] = data['cylindernumber'].apply(lambda x : text2int(x))\n",
        "data['fuelsystem'] = encoder2.fit_transform(encoded_x3)\n",
        "data.head()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fueltype</th>\n",
              "      <th>aspiration</th>\n",
              "      <th>drivewheel</th>\n",
              "      <th>enginelocation</th>\n",
              "      <th>wheelbase</th>\n",
              "      <th>carlength</th>\n",
              "      <th>carwidth</th>\n",
              "      <th>carheight</th>\n",
              "      <th>curbweight</th>\n",
              "      <th>enginetype</th>\n",
              "      <th>cylindernumber</th>\n",
              "      <th>enginesize</th>\n",
              "      <th>fuelsystem</th>\n",
              "      <th>boreratio</th>\n",
              "      <th>stroke</th>\n",
              "      <th>compressionratio</th>\n",
              "      <th>peakrpm</th>\n",
              "      <th>horsepower</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>88.6</td>\n",
              "      <td>168.8</td>\n",
              "      <td>64.1</td>\n",
              "      <td>48.8</td>\n",
              "      <td>2548</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>130</td>\n",
              "      <td>5</td>\n",
              "      <td>3.47</td>\n",
              "      <td>2.68</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5000</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>88.6</td>\n",
              "      <td>168.8</td>\n",
              "      <td>64.1</td>\n",
              "      <td>48.8</td>\n",
              "      <td>2548</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>130</td>\n",
              "      <td>5</td>\n",
              "      <td>3.47</td>\n",
              "      <td>2.68</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5000</td>\n",
              "      <td>111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>94.5</td>\n",
              "      <td>171.2</td>\n",
              "      <td>65.5</td>\n",
              "      <td>52.4</td>\n",
              "      <td>2823</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>152</td>\n",
              "      <td>5</td>\n",
              "      <td>2.68</td>\n",
              "      <td>3.47</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5000</td>\n",
              "      <td>154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>99.8</td>\n",
              "      <td>176.6</td>\n",
              "      <td>66.2</td>\n",
              "      <td>54.3</td>\n",
              "      <td>2337</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>109</td>\n",
              "      <td>5</td>\n",
              "      <td>3.19</td>\n",
              "      <td>3.40</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5500</td>\n",
              "      <td>102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>99.4</td>\n",
              "      <td>176.6</td>\n",
              "      <td>66.4</td>\n",
              "      <td>54.3</td>\n",
              "      <td>2824</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>136</td>\n",
              "      <td>5</td>\n",
              "      <td>3.19</td>\n",
              "      <td>3.40</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5500</td>\n",
              "      <td>115</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fueltype  aspiration  drivewheel  ...  compressionratio  peakrpm  horsepower\n",
              "0         0           0           2  ...               9.0     5000         111\n",
              "1         0           0           2  ...               9.0     5000         111\n",
              "2         0           0           2  ...               9.0     5000         154\n",
              "3         0           0           1  ...              10.0     5500         102\n",
              "4         0           0           0  ...               8.0     5500         115\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPV05V58V-pT"
      },
      "source": [
        "# Start of training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAVbDHv55EHx"
      },
      "source": [
        "drop ount the columns that need one-hot encoder\n",
        "replace the tranformer by dummy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvEWmhdwR9qE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c49bf8d-1781-4086-9561-43b95dda7fe6"
      },
      "source": [
        "data.drop('drivewheel',axis = 1,inplace = True)\n",
        "data.drop('fuelsystem',axis = 1,inplace = True)\n",
        "data.drop('enginetype',axis = 1,inplace = True)\n",
        "\n",
        "data.info()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 205 entries, 0 to 204\n",
            "Data columns (total 15 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   fueltype          205 non-null    int64  \n",
            " 1   aspiration        205 non-null    int64  \n",
            " 2   enginelocation    205 non-null    int64  \n",
            " 3   wheelbase         205 non-null    float64\n",
            " 4   carlength         205 non-null    float64\n",
            " 5   carwidth          205 non-null    float64\n",
            " 6   carheight         205 non-null    float64\n",
            " 7   curbweight        205 non-null    int64  \n",
            " 8   cylindernumber    205 non-null    int64  \n",
            " 9   enginesize        205 non-null    int64  \n",
            " 10  boreratio         205 non-null    float64\n",
            " 11  stroke            205 non-null    float64\n",
            " 12  compressionratio  205 non-null    float64\n",
            " 13  peakrpm           205 non-null    int64  \n",
            " 14  horsepower        205 non-null    int64  \n",
            "dtypes: float64(7), int64(8)\n",
            "memory usage: 24.1 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9M6qc_d5R8s"
      },
      "source": [
        "concat the 3 dummy columns with the rest of the float columns "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fcUrw5VV7cI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "53739d8e-3bb4-4fd3-c3bc-06e935ff2410"
      },
      "source": [
        "dataset = data.values\n",
        "x4 = dataset[:, 0:14 ]\n",
        "xs = [dummy_x1, dummy_x2, dummy_x3, x4]\n",
        "\n",
        "result = pd.concat([pd.DataFrame(dummy_x1), pd.DataFrame(dummy_x2),pd.DataFrame(dummy_x3), pd.DataFrame(x4)], axis=1, ignore_index = True)\n",
        "#x_train = pd.concat(xs,axis=1, join=\"inner\")\n",
        "result"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>88.6</td>\n",
              "      <td>168.8</td>\n",
              "      <td>64.1</td>\n",
              "      <td>48.8</td>\n",
              "      <td>2548.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>3.47</td>\n",
              "      <td>2.68</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>88.6</td>\n",
              "      <td>168.8</td>\n",
              "      <td>64.1</td>\n",
              "      <td>48.8</td>\n",
              "      <td>2548.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>3.47</td>\n",
              "      <td>2.68</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>94.5</td>\n",
              "      <td>171.2</td>\n",
              "      <td>65.5</td>\n",
              "      <td>52.4</td>\n",
              "      <td>2823.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>2.68</td>\n",
              "      <td>3.47</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.8</td>\n",
              "      <td>176.6</td>\n",
              "      <td>66.2</td>\n",
              "      <td>54.3</td>\n",
              "      <td>2337.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>109.0</td>\n",
              "      <td>3.19</td>\n",
              "      <td>3.40</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>99.4</td>\n",
              "      <td>176.6</td>\n",
              "      <td>66.4</td>\n",
              "      <td>54.3</td>\n",
              "      <td>2824.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>3.19</td>\n",
              "      <td>3.40</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>109.1</td>\n",
              "      <td>188.8</td>\n",
              "      <td>68.9</td>\n",
              "      <td>55.5</td>\n",
              "      <td>2952.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>3.78</td>\n",
              "      <td>3.15</td>\n",
              "      <td>9.5</td>\n",
              "      <td>5400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>109.1</td>\n",
              "      <td>188.8</td>\n",
              "      <td>68.8</td>\n",
              "      <td>55.5</td>\n",
              "      <td>3049.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>3.78</td>\n",
              "      <td>3.15</td>\n",
              "      <td>8.7</td>\n",
              "      <td>5300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>109.1</td>\n",
              "      <td>188.8</td>\n",
              "      <td>68.9</td>\n",
              "      <td>55.5</td>\n",
              "      <td>3012.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>173.0</td>\n",
              "      <td>3.58</td>\n",
              "      <td>2.87</td>\n",
              "      <td>8.8</td>\n",
              "      <td>5500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>109.1</td>\n",
              "      <td>188.8</td>\n",
              "      <td>68.9</td>\n",
              "      <td>55.5</td>\n",
              "      <td>3217.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>3.01</td>\n",
              "      <td>3.40</td>\n",
              "      <td>23.0</td>\n",
              "      <td>4800.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>109.1</td>\n",
              "      <td>188.8</td>\n",
              "      <td>68.9</td>\n",
              "      <td>55.5</td>\n",
              "      <td>3062.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>3.78</td>\n",
              "      <td>3.15</td>\n",
              "      <td>9.5</td>\n",
              "      <td>5400.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>205 rows × 32 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      0    1    2    3    4    5   ...   26     27    28    29    30      31\n",
              "0    0.0  0.0  1.0  1.0  0.0  0.0  ...  4.0  130.0  3.47  2.68   9.0  5000.0\n",
              "1    0.0  0.0  1.0  1.0  0.0  0.0  ...  4.0  130.0  3.47  2.68   9.0  5000.0\n",
              "2    0.0  0.0  1.0  0.0  0.0  0.0  ...  6.0  152.0  2.68  3.47   9.0  5000.0\n",
              "3    0.0  1.0  0.0  0.0  0.0  0.0  ...  4.0  109.0  3.19  3.40  10.0  5500.0\n",
              "4    1.0  0.0  0.0  0.0  0.0  0.0  ...  5.0  136.0  3.19  3.40   8.0  5500.0\n",
              "..   ...  ...  ...  ...  ...  ...  ...  ...    ...   ...   ...   ...     ...\n",
              "200  0.0  0.0  1.0  0.0  0.0  0.0  ...  4.0  141.0  3.78  3.15   9.5  5400.0\n",
              "201  0.0  0.0  1.0  0.0  0.0  0.0  ...  4.0  141.0  3.78  3.15   8.7  5300.0\n",
              "202  0.0  0.0  1.0  0.0  0.0  0.0  ...  6.0  173.0  3.58  2.87   8.8  5500.0\n",
              "203  0.0  0.0  1.0  0.0  0.0  0.0  ...  6.0  145.0  3.01  3.40  23.0  4800.0\n",
              "204  0.0  0.0  1.0  0.0  0.0  0.0  ...  4.0  141.0  3.78  3.15   9.5  5400.0\n",
              "\n",
              "[205 rows x 32 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K4wE71q5fru"
      },
      "source": [
        "creat the model, devide the data into X and Y (train and test) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjDaiCdHYtmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6319b000-d68c-4dff-c550-7a3d11c86e93"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = result.astype(float)\n",
        "dataset = data.values\n",
        "y = pd.DataFrame(dataset[:,14].astype(float))\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.2, random_state = 2)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(40,input_dim = 32, activation = 'relu' ))\n",
        "model.add(Dense(20, activation = 'relu' ))\n",
        "model.add(Dense(40, activation = 'relu' ))\n",
        "model.add(Dense(20, activation = 'relu' ))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "model.compile( loss='mean_squared_error', optimizer='adam', metrics = ['mse'])\n",
        "\n",
        "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=700, batch_size=16)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/700\n",
            "11/11 [==============================] - 1s 26ms/step - loss: 26386.7285 - mse: 26386.7285 - val_loss: 8915.1992 - val_mse: 8915.1992\n",
            "Epoch 2/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5315.7285 - mse: 5315.7285 - val_loss: 1108.5936 - val_mse: 1108.5936\n",
            "Epoch 3/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2344.3052 - mse: 2344.3052 - val_loss: 889.0262 - val_mse: 889.0262\n",
            "Epoch 4/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1533.2256 - mse: 1533.2256 - val_loss: 1152.9242 - val_mse: 1152.9242\n",
            "Epoch 5/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 921.9536 - mse: 921.9536 - val_loss: 666.3121 - val_mse: 666.3121\n",
            "Epoch 6/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 817.7664 - mse: 817.7664 - val_loss: 700.3318 - val_mse: 700.3318\n",
            "Epoch 7/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 731.6266 - mse: 731.6266 - val_loss: 615.6440 - val_mse: 615.6440\n",
            "Epoch 8/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 708.7548 - mse: 708.7548 - val_loss: 622.6360 - val_mse: 622.6360\n",
            "Epoch 9/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 697.1685 - mse: 697.1685 - val_loss: 626.1591 - val_mse: 626.1591\n",
            "Epoch 10/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 707.9001 - mse: 707.9001 - val_loss: 620.7841 - val_mse: 620.7841\n",
            "Epoch 11/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 695.2427 - mse: 695.2427 - val_loss: 627.9522 - val_mse: 627.9522\n",
            "Epoch 12/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 682.3201 - mse: 682.3201 - val_loss: 630.8619 - val_mse: 630.8619\n",
            "Epoch 13/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 686.4471 - mse: 686.4471 - val_loss: 627.3653 - val_mse: 627.3653\n",
            "Epoch 14/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 678.6392 - mse: 678.6392 - val_loss: 654.4309 - val_mse: 654.4309\n",
            "Epoch 15/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 689.9217 - mse: 689.9217 - val_loss: 663.7278 - val_mse: 663.7278\n",
            "Epoch 16/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 711.7263 - mse: 711.7263 - val_loss: 610.3547 - val_mse: 610.3547\n",
            "Epoch 17/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 677.7944 - mse: 677.7944 - val_loss: 622.7134 - val_mse: 622.7134\n",
            "Epoch 18/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 708.5155 - mse: 708.5155 - val_loss: 780.7975 - val_mse: 780.7975\n",
            "Epoch 19/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 839.4353 - mse: 839.4353 - val_loss: 604.8174 - val_mse: 604.8174\n",
            "Epoch 20/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 673.1249 - mse: 673.1249 - val_loss: 611.9064 - val_mse: 611.9064\n",
            "Epoch 21/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 694.4553 - mse: 694.4553 - val_loss: 622.3029 - val_mse: 622.3029\n",
            "Epoch 22/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 683.0220 - mse: 683.0220 - val_loss: 600.2182 - val_mse: 600.2182\n",
            "Epoch 23/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 655.4284 - mse: 655.4284 - val_loss: 642.6196 - val_mse: 642.6196\n",
            "Epoch 24/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 685.4413 - mse: 685.4413 - val_loss: 600.5391 - val_mse: 600.5391\n",
            "Epoch 25/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 664.6219 - mse: 664.6219 - val_loss: 616.3872 - val_mse: 616.3872\n",
            "Epoch 26/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 695.6237 - mse: 695.6237 - val_loss: 630.2314 - val_mse: 630.2314\n",
            "Epoch 27/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 664.3076 - mse: 664.3076 - val_loss: 616.7974 - val_mse: 616.7974\n",
            "Epoch 28/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 651.9150 - mse: 651.9150 - val_loss: 612.9079 - val_mse: 612.9079\n",
            "Epoch 29/700\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 662.1647 - mse: 662.1647 - val_loss: 609.1320 - val_mse: 609.1320\n",
            "Epoch 30/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 721.3735 - mse: 721.3735 - val_loss: 666.7690 - val_mse: 666.7690\n",
            "Epoch 31/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 651.7703 - mse: 651.7703 - val_loss: 590.7731 - val_mse: 590.7731\n",
            "Epoch 32/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 640.3521 - mse: 640.3521 - val_loss: 591.5092 - val_mse: 591.5092\n",
            "Epoch 33/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 675.1675 - mse: 675.1675 - val_loss: 598.8807 - val_mse: 598.8807\n",
            "Epoch 34/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 648.6943 - mse: 648.6943 - val_loss: 615.7795 - val_mse: 615.7795\n",
            "Epoch 35/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 781.7543 - mse: 781.7543 - val_loss: 585.9375 - val_mse: 585.9375\n",
            "Epoch 36/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 646.8607 - mse: 646.8607 - val_loss: 579.4361 - val_mse: 579.4361\n",
            "Epoch 37/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 631.5718 - mse: 631.5718 - val_loss: 741.8240 - val_mse: 741.8240\n",
            "Epoch 38/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 657.1053 - mse: 657.1053 - val_loss: 578.2139 - val_mse: 578.2139\n",
            "Epoch 39/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 659.9778 - mse: 659.9778 - val_loss: 585.1316 - val_mse: 585.1316\n",
            "Epoch 40/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 618.8283 - mse: 618.8283 - val_loss: 568.9109 - val_mse: 568.9109\n",
            "Epoch 41/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 648.6403 - mse: 648.6403 - val_loss: 657.1132 - val_mse: 657.1132\n",
            "Epoch 42/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 673.1088 - mse: 673.1088 - val_loss: 705.5396 - val_mse: 705.5396\n",
            "Epoch 43/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 667.2629 - mse: 667.2629 - val_loss: 700.4513 - val_mse: 700.4513\n",
            "Epoch 44/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 638.8240 - mse: 638.8240 - val_loss: 641.5732 - val_mse: 641.5732\n",
            "Epoch 45/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 646.2871 - mse: 646.2871 - val_loss: 617.4517 - val_mse: 617.4517\n",
            "Epoch 46/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 613.7550 - mse: 613.7550 - val_loss: 576.4588 - val_mse: 576.4588\n",
            "Epoch 47/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 654.3157 - mse: 654.3157 - val_loss: 572.7096 - val_mse: 572.7096\n",
            "Epoch 48/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 713.0490 - mse: 713.0490 - val_loss: 558.6299 - val_mse: 558.6299\n",
            "Epoch 49/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 618.0945 - mse: 618.0945 - val_loss: 582.9484 - val_mse: 582.9484\n",
            "Epoch 50/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 611.7303 - mse: 611.7303 - val_loss: 621.3989 - val_mse: 621.3989\n",
            "Epoch 51/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 593.3753 - mse: 593.3753 - val_loss: 553.5990 - val_mse: 553.5990\n",
            "Epoch 52/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 693.8521 - mse: 693.8521 - val_loss: 561.1119 - val_mse: 561.1119\n",
            "Epoch 53/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 745.6790 - mse: 745.6790 - val_loss: 804.5360 - val_mse: 804.5360\n",
            "Epoch 54/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 664.4304 - mse: 664.4304 - val_loss: 644.1725 - val_mse: 644.1725\n",
            "Epoch 55/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 626.7103 - mse: 626.7103 - val_loss: 676.4511 - val_mse: 676.4511\n",
            "Epoch 56/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 623.9733 - mse: 623.9733 - val_loss: 528.0446 - val_mse: 528.0446\n",
            "Epoch 57/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 581.3104 - mse: 581.3104 - val_loss: 512.7016 - val_mse: 512.7016\n",
            "Epoch 58/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 628.2258 - mse: 628.2258 - val_loss: 540.5627 - val_mse: 540.5627\n",
            "Epoch 59/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 594.5923 - mse: 594.5923 - val_loss: 575.0837 - val_mse: 575.0837\n",
            "Epoch 60/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 611.6166 - mse: 611.6166 - val_loss: 830.9337 - val_mse: 830.9337\n",
            "Epoch 61/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 683.0106 - mse: 683.0106 - val_loss: 591.4996 - val_mse: 591.4996\n",
            "Epoch 62/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 550.0122 - mse: 550.0122 - val_loss: 523.1899 - val_mse: 523.1899\n",
            "Epoch 63/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 560.3687 - mse: 560.3687 - val_loss: 813.1303 - val_mse: 813.1303\n",
            "Epoch 64/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 622.6606 - mse: 622.6606 - val_loss: 609.0652 - val_mse: 609.0652\n",
            "Epoch 65/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 624.4267 - mse: 624.4267 - val_loss: 574.2164 - val_mse: 574.2164\n",
            "Epoch 66/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 555.9348 - mse: 555.9348 - val_loss: 521.5098 - val_mse: 521.5098\n",
            "Epoch 67/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 566.2139 - mse: 566.2139 - val_loss: 497.3320 - val_mse: 497.3320\n",
            "Epoch 68/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 567.1702 - mse: 567.1702 - val_loss: 565.3190 - val_mse: 565.3190\n",
            "Epoch 69/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 649.7993 - mse: 649.7993 - val_loss: 554.9164 - val_mse: 554.9164\n",
            "Epoch 70/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 567.6087 - mse: 567.6087 - val_loss: 514.9470 - val_mse: 514.9470\n",
            "Epoch 71/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 559.4426 - mse: 559.4426 - val_loss: 499.9065 - val_mse: 499.9065\n",
            "Epoch 72/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 534.0609 - mse: 534.0609 - val_loss: 702.2202 - val_mse: 702.2202\n",
            "Epoch 73/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 592.2256 - mse: 592.2256 - val_loss: 627.0596 - val_mse: 627.0596\n",
            "Epoch 74/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 587.2227 - mse: 587.2227 - val_loss: 515.3889 - val_mse: 515.3889\n",
            "Epoch 75/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 510.9489 - mse: 510.9489 - val_loss: 927.4025 - val_mse: 927.4025\n",
            "Epoch 76/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 869.2822 - mse: 869.2822 - val_loss: 503.7967 - val_mse: 503.7967\n",
            "Epoch 77/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 537.9718 - mse: 537.9718 - val_loss: 525.4147 - val_mse: 525.4147\n",
            "Epoch 78/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 599.7563 - mse: 599.7563 - val_loss: 491.1446 - val_mse: 491.1446\n",
            "Epoch 79/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 589.9325 - mse: 589.9325 - val_loss: 538.3371 - val_mse: 538.3371\n",
            "Epoch 80/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 545.6700 - mse: 545.6700 - val_loss: 495.4534 - val_mse: 495.4534\n",
            "Epoch 81/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 689.6166 - mse: 689.6166 - val_loss: 461.0951 - val_mse: 461.0951\n",
            "Epoch 82/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 798.5303 - mse: 798.5303 - val_loss: 483.9036 - val_mse: 483.9036\n",
            "Epoch 83/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 580.1131 - mse: 580.1131 - val_loss: 559.0292 - val_mse: 559.0292\n",
            "Epoch 84/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 524.0936 - mse: 524.0936 - val_loss: 576.1000 - val_mse: 576.1000\n",
            "Epoch 85/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 525.8522 - mse: 525.8522 - val_loss: 493.1366 - val_mse: 493.1366\n",
            "Epoch 86/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 534.8987 - mse: 534.8987 - val_loss: 441.6613 - val_mse: 441.6613\n",
            "Epoch 87/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 527.8365 - mse: 527.8365 - val_loss: 448.1657 - val_mse: 448.1657\n",
            "Epoch 88/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 481.0041 - mse: 481.0041 - val_loss: 495.0776 - val_mse: 495.0776\n",
            "Epoch 89/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 540.3567 - mse: 540.3567 - val_loss: 537.5900 - val_mse: 537.5900\n",
            "Epoch 90/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 497.2643 - mse: 497.2643 - val_loss: 430.9425 - val_mse: 430.9425\n",
            "Epoch 91/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 462.4592 - mse: 462.4592 - val_loss: 415.2116 - val_mse: 415.2116\n",
            "Epoch 92/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 516.3150 - mse: 516.3150 - val_loss: 421.9086 - val_mse: 421.9086\n",
            "Epoch 93/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 655.6833 - mse: 655.6833 - val_loss: 422.2088 - val_mse: 422.2088\n",
            "Epoch 94/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 613.5780 - mse: 613.5780 - val_loss: 491.1631 - val_mse: 491.1631\n",
            "Epoch 95/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 490.2981 - mse: 490.2981 - val_loss: 587.9602 - val_mse: 587.9602\n",
            "Epoch 96/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 456.2704 - mse: 456.2704 - val_loss: 413.7911 - val_mse: 413.7911\n",
            "Epoch 97/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 467.0928 - mse: 467.0928 - val_loss: 410.4254 - val_mse: 410.4254\n",
            "Epoch 98/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 440.6794 - mse: 440.6794 - val_loss: 433.3249 - val_mse: 433.3249\n",
            "Epoch 99/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 462.3297 - mse: 462.3297 - val_loss: 522.9594 - val_mse: 522.9594\n",
            "Epoch 100/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 568.4437 - mse: 568.4437 - val_loss: 743.6102 - val_mse: 743.6102\n",
            "Epoch 101/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 512.2513 - mse: 512.2513 - val_loss: 581.1023 - val_mse: 581.1023\n",
            "Epoch 102/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 487.0679 - mse: 487.0679 - val_loss: 426.0713 - val_mse: 426.0713\n",
            "Epoch 103/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 425.1921 - mse: 425.1921 - val_loss: 392.7393 - val_mse: 392.7393\n",
            "Epoch 104/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 444.7469 - mse: 444.7469 - val_loss: 404.5780 - val_mse: 404.5780\n",
            "Epoch 105/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 448.0993 - mse: 448.0993 - val_loss: 379.6947 - val_mse: 379.6947\n",
            "Epoch 106/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 398.6459 - mse: 398.6459 - val_loss: 462.7122 - val_mse: 462.7122\n",
            "Epoch 107/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 444.7120 - mse: 444.7120 - val_loss: 634.2208 - val_mse: 634.2208\n",
            "Epoch 108/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 448.0498 - mse: 448.0498 - val_loss: 373.9018 - val_mse: 373.9018\n",
            "Epoch 109/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 447.3389 - mse: 447.3389 - val_loss: 379.1955 - val_mse: 379.1955\n",
            "Epoch 110/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 428.1770 - mse: 428.1770 - val_loss: 459.4143 - val_mse: 459.4143\n",
            "Epoch 111/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 564.0806 - mse: 564.0806 - val_loss: 404.4772 - val_mse: 404.4772\n",
            "Epoch 112/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 478.2541 - mse: 478.2541 - val_loss: 407.2711 - val_mse: 407.2711\n",
            "Epoch 113/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 523.7056 - mse: 523.7056 - val_loss: 356.0042 - val_mse: 356.0042\n",
            "Epoch 114/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 489.2149 - mse: 489.2149 - val_loss: 438.1663 - val_mse: 438.1663\n",
            "Epoch 115/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 402.6334 - mse: 402.6334 - val_loss: 668.6674 - val_mse: 668.6674\n",
            "Epoch 116/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 412.6324 - mse: 412.6324 - val_loss: 359.5294 - val_mse: 359.5294\n",
            "Epoch 117/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 369.7316 - mse: 369.7316 - val_loss: 333.5749 - val_mse: 333.5749\n",
            "Epoch 118/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 393.9019 - mse: 393.9019 - val_loss: 352.8013 - val_mse: 352.8013\n",
            "Epoch 119/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 420.1758 - mse: 420.1758 - val_loss: 362.5133 - val_mse: 362.5133\n",
            "Epoch 120/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 422.2388 - mse: 422.2388 - val_loss: 322.5309 - val_mse: 322.5309\n",
            "Epoch 121/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 388.0788 - mse: 388.0788 - val_loss: 325.9827 - val_mse: 325.9827\n",
            "Epoch 122/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 402.4113 - mse: 402.4113 - val_loss: 349.4799 - val_mse: 349.4799\n",
            "Epoch 123/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 382.2614 - mse: 382.2614 - val_loss: 323.1127 - val_mse: 323.1127\n",
            "Epoch 124/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 378.2909 - mse: 378.2909 - val_loss: 485.3708 - val_mse: 485.3708\n",
            "Epoch 125/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 437.5993 - mse: 437.5993 - val_loss: 307.7234 - val_mse: 307.7234\n",
            "Epoch 126/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 368.2493 - mse: 368.2493 - val_loss: 320.9583 - val_mse: 320.9583\n",
            "Epoch 127/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 366.9883 - mse: 366.9883 - val_loss: 290.3138 - val_mse: 290.3138\n",
            "Epoch 128/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 345.2218 - mse: 345.2218 - val_loss: 280.9253 - val_mse: 280.9253\n",
            "Epoch 129/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 345.7342 - mse: 345.7342 - val_loss: 309.7315 - val_mse: 309.7315\n",
            "Epoch 130/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 326.1325 - mse: 326.1325 - val_loss: 287.1039 - val_mse: 287.1039\n",
            "Epoch 131/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 329.3049 - mse: 329.3049 - val_loss: 345.0342 - val_mse: 345.0342\n",
            "Epoch 132/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 360.2323 - mse: 360.2323 - val_loss: 490.1605 - val_mse: 490.1605\n",
            "Epoch 133/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 418.2046 - mse: 418.2046 - val_loss: 361.3573 - val_mse: 361.3573\n",
            "Epoch 134/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 398.4160 - mse: 398.4160 - val_loss: 292.6047 - val_mse: 292.6047\n",
            "Epoch 135/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 556.9915 - mse: 556.9915 - val_loss: 492.6680 - val_mse: 492.6680\n",
            "Epoch 136/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 455.4000 - mse: 455.4000 - val_loss: 322.9503 - val_mse: 322.9503\n",
            "Epoch 137/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 353.3839 - mse: 353.3839 - val_loss: 278.0164 - val_mse: 278.0164\n",
            "Epoch 138/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 320.1204 - mse: 320.1204 - val_loss: 239.4442 - val_mse: 239.4442\n",
            "Epoch 139/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 430.2341 - mse: 430.2341 - val_loss: 290.7736 - val_mse: 290.7736\n",
            "Epoch 140/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 305.3987 - mse: 305.3987 - val_loss: 273.6408 - val_mse: 273.6408\n",
            "Epoch 141/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 299.4966 - mse: 299.4966 - val_loss: 333.6246 - val_mse: 333.6246\n",
            "Epoch 142/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 305.3363 - mse: 305.3363 - val_loss: 285.5801 - val_mse: 285.5801\n",
            "Epoch 143/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 338.7128 - mse: 338.7128 - val_loss: 286.2293 - val_mse: 286.2293\n",
            "Epoch 144/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 393.7949 - mse: 393.7949 - val_loss: 540.2400 - val_mse: 540.2400\n",
            "Epoch 145/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 531.7911 - mse: 531.7911 - val_loss: 302.7810 - val_mse: 302.7810\n",
            "Epoch 146/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 337.9190 - mse: 337.9190 - val_loss: 346.9588 - val_mse: 346.9588\n",
            "Epoch 147/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 381.8539 - mse: 381.8539 - val_loss: 268.1606 - val_mse: 268.1606\n",
            "Epoch 148/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 358.2533 - mse: 358.2533 - val_loss: 232.6183 - val_mse: 232.6183\n",
            "Epoch 149/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 300.9564 - mse: 300.9564 - val_loss: 245.5088 - val_mse: 245.5088\n",
            "Epoch 150/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 335.9359 - mse: 335.9359 - val_loss: 302.6701 - val_mse: 302.6701\n",
            "Epoch 151/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 338.9833 - mse: 338.9833 - val_loss: 434.5384 - val_mse: 434.5384\n",
            "Epoch 152/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 331.3452 - mse: 331.3452 - val_loss: 308.4453 - val_mse: 308.4453\n",
            "Epoch 153/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 313.1645 - mse: 313.1645 - val_loss: 239.5854 - val_mse: 239.5854\n",
            "Epoch 154/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 269.7515 - mse: 269.7515 - val_loss: 278.1138 - val_mse: 278.1138\n",
            "Epoch 155/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 336.0533 - mse: 336.0533 - val_loss: 262.8241 - val_mse: 262.8241\n",
            "Epoch 156/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 299.5505 - mse: 299.5505 - val_loss: 194.5037 - val_mse: 194.5037\n",
            "Epoch 157/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 311.9724 - mse: 311.9724 - val_loss: 210.4753 - val_mse: 210.4753\n",
            "Epoch 158/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 341.4604 - mse: 341.4604 - val_loss: 216.5950 - val_mse: 216.5950\n",
            "Epoch 159/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 287.2550 - mse: 287.2550 - val_loss: 196.1639 - val_mse: 196.1639\n",
            "Epoch 160/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 274.6393 - mse: 274.6393 - val_loss: 201.2328 - val_mse: 201.2328\n",
            "Epoch 161/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 264.2721 - mse: 264.2721 - val_loss: 284.8673 - val_mse: 284.8673\n",
            "Epoch 162/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 268.7825 - mse: 268.7825 - val_loss: 402.4828 - val_mse: 402.4828\n",
            "Epoch 163/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 328.9537 - mse: 328.9537 - val_loss: 176.5761 - val_mse: 176.5761\n",
            "Epoch 164/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 254.4724 - mse: 254.4724 - val_loss: 195.6429 - val_mse: 195.6429\n",
            "Epoch 165/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 243.1858 - mse: 243.1858 - val_loss: 173.3656 - val_mse: 173.3656\n",
            "Epoch 166/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 267.3607 - mse: 267.3607 - val_loss: 182.3503 - val_mse: 182.3503\n",
            "Epoch 167/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 286.5230 - mse: 286.5230 - val_loss: 171.7100 - val_mse: 171.7100\n",
            "Epoch 168/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 248.3607 - mse: 248.3607 - val_loss: 227.9124 - val_mse: 227.9124\n",
            "Epoch 169/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 298.6235 - mse: 298.6235 - val_loss: 306.9899 - val_mse: 306.9899\n",
            "Epoch 170/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 254.2146 - mse: 254.2146 - val_loss: 205.3468 - val_mse: 205.3468\n",
            "Epoch 171/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 247.6377 - mse: 247.6377 - val_loss: 167.4406 - val_mse: 167.4406\n",
            "Epoch 172/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 270.2918 - mse: 270.2918 - val_loss: 153.4273 - val_mse: 153.4273\n",
            "Epoch 173/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 274.2458 - mse: 274.2458 - val_loss: 162.4714 - val_mse: 162.4714\n",
            "Epoch 174/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 252.4262 - mse: 252.4262 - val_loss: 227.7311 - val_mse: 227.7311\n",
            "Epoch 175/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 232.2327 - mse: 232.2327 - val_loss: 603.9059 - val_mse: 603.9059\n",
            "Epoch 176/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 492.8254 - mse: 492.8254 - val_loss: 412.4370 - val_mse: 412.4370\n",
            "Epoch 177/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 238.7108 - mse: 238.7108 - val_loss: 308.2186 - val_mse: 308.2186\n",
            "Epoch 178/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 352.2208 - mse: 352.2208 - val_loss: 163.1897 - val_mse: 163.1897\n",
            "Epoch 179/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 237.8317 - mse: 237.8317 - val_loss: 151.0979 - val_mse: 151.0979\n",
            "Epoch 180/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 245.3663 - mse: 245.3663 - val_loss: 241.1606 - val_mse: 241.1606\n",
            "Epoch 181/700\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 298.3131 - mse: 298.3131 - val_loss: 188.4739 - val_mse: 188.4739\n",
            "Epoch 182/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 265.0185 - mse: 265.0185 - val_loss: 157.4919 - val_mse: 157.4919\n",
            "Epoch 183/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 250.4156 - mse: 250.4156 - val_loss: 158.5785 - val_mse: 158.5785\n",
            "Epoch 184/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 229.0557 - mse: 229.0557 - val_loss: 127.5690 - val_mse: 127.5690\n",
            "Epoch 185/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 226.5329 - mse: 226.5329 - val_loss: 119.4722 - val_mse: 119.4722\n",
            "Epoch 186/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 275.2570 - mse: 275.2570 - val_loss: 418.1527 - val_mse: 418.1527\n",
            "Epoch 187/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 295.9318 - mse: 295.9318 - val_loss: 137.6976 - val_mse: 137.6976\n",
            "Epoch 188/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 214.6794 - mse: 214.6794 - val_loss: 175.5776 - val_mse: 175.5776\n",
            "Epoch 189/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 278.8528 - mse: 278.8528 - val_loss: 136.9736 - val_mse: 136.9736\n",
            "Epoch 190/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 222.7618 - mse: 222.7618 - val_loss: 139.2195 - val_mse: 139.2195\n",
            "Epoch 191/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 245.3849 - mse: 245.3849 - val_loss: 178.7581 - val_mse: 178.7581\n",
            "Epoch 192/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 288.1653 - mse: 288.1653 - val_loss: 177.5198 - val_mse: 177.5198\n",
            "Epoch 193/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 277.6777 - mse: 277.6777 - val_loss: 110.0362 - val_mse: 110.0362\n",
            "Epoch 194/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 216.8716 - mse: 216.8716 - val_loss: 108.5579 - val_mse: 108.5579\n",
            "Epoch 195/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 227.9598 - mse: 227.9598 - val_loss: 151.8417 - val_mse: 151.8417\n",
            "Epoch 196/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 259.8431 - mse: 259.8431 - val_loss: 265.6782 - val_mse: 265.6782\n",
            "Epoch 197/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 234.8020 - mse: 234.8020 - val_loss: 125.7542 - val_mse: 125.7542\n",
            "Epoch 198/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 221.4659 - mse: 221.4659 - val_loss: 91.0244 - val_mse: 91.0244\n",
            "Epoch 199/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 209.0405 - mse: 209.0405 - val_loss: 96.3243 - val_mse: 96.3243\n",
            "Epoch 200/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 209.1655 - mse: 209.1655 - val_loss: 101.5016 - val_mse: 101.5016\n",
            "Epoch 201/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 198.1101 - mse: 198.1101 - val_loss: 142.9242 - val_mse: 142.9242\n",
            "Epoch 202/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 307.5438 - mse: 307.5438 - val_loss: 90.1502 - val_mse: 90.1502\n",
            "Epoch 203/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 357.3215 - mse: 357.3215 - val_loss: 165.5978 - val_mse: 165.5978\n",
            "Epoch 204/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 241.2319 - mse: 241.2319 - val_loss: 160.6072 - val_mse: 160.6072\n",
            "Epoch 205/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 285.1278 - mse: 285.1278 - val_loss: 380.8673 - val_mse: 380.8673\n",
            "Epoch 206/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 408.3840 - mse: 408.3840 - val_loss: 128.0134 - val_mse: 128.0134\n",
            "Epoch 207/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 262.5283 - mse: 262.5283 - val_loss: 137.3926 - val_mse: 137.3926\n",
            "Epoch 208/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 248.2702 - mse: 248.2702 - val_loss: 124.2440 - val_mse: 124.2440\n",
            "Epoch 209/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 223.5551 - mse: 223.5551 - val_loss: 142.0337 - val_mse: 142.0337\n",
            "Epoch 210/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 221.0586 - mse: 221.0586 - val_loss: 235.8088 - val_mse: 235.8088\n",
            "Epoch 211/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 278.4363 - mse: 278.4363 - val_loss: 126.0426 - val_mse: 126.0426\n",
            "Epoch 212/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 259.9660 - mse: 259.9660 - val_loss: 384.3874 - val_mse: 384.3874\n",
            "Epoch 213/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 293.1133 - mse: 293.1133 - val_loss: 268.1422 - val_mse: 268.1422\n",
            "Epoch 214/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 238.4618 - mse: 238.4618 - val_loss: 175.5427 - val_mse: 175.5427\n",
            "Epoch 215/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 250.5694 - mse: 250.5694 - val_loss: 122.8747 - val_mse: 122.8747\n",
            "Epoch 216/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 242.2241 - mse: 242.2241 - val_loss: 106.7481 - val_mse: 106.7481\n",
            "Epoch 217/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 226.1462 - mse: 226.1462 - val_loss: 102.7286 - val_mse: 102.7286\n",
            "Epoch 218/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 221.9564 - mse: 221.9564 - val_loss: 114.0218 - val_mse: 114.0218\n",
            "Epoch 219/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 254.6653 - mse: 254.6653 - val_loss: 204.4688 - val_mse: 204.4688\n",
            "Epoch 220/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 255.5471 - mse: 255.5471 - val_loss: 196.4656 - val_mse: 196.4656\n",
            "Epoch 221/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 306.9686 - mse: 306.9686 - val_loss: 250.2270 - val_mse: 250.2270\n",
            "Epoch 222/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 341.9790 - mse: 341.9790 - val_loss: 253.2712 - val_mse: 253.2712\n",
            "Epoch 223/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 320.1064 - mse: 320.1064 - val_loss: 364.3107 - val_mse: 364.3107\n",
            "Epoch 224/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 312.8478 - mse: 312.8478 - val_loss: 111.2254 - val_mse: 111.2254\n",
            "Epoch 225/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 228.7147 - mse: 228.7147 - val_loss: 97.3758 - val_mse: 97.3758\n",
            "Epoch 226/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 227.0130 - mse: 227.0130 - val_loss: 95.7583 - val_mse: 95.7583\n",
            "Epoch 227/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 206.9274 - mse: 206.9274 - val_loss: 93.6998 - val_mse: 93.6998\n",
            "Epoch 228/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 225.6526 - mse: 225.6526 - val_loss: 325.7821 - val_mse: 325.7821\n",
            "Epoch 229/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 271.9477 - mse: 271.9477 - val_loss: 100.2270 - val_mse: 100.2270\n",
            "Epoch 230/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 227.3574 - mse: 227.3574 - val_loss: 94.7269 - val_mse: 94.7269\n",
            "Epoch 231/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 226.6849 - mse: 226.6849 - val_loss: 117.4298 - val_mse: 117.4298\n",
            "Epoch 232/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 219.5575 - mse: 219.5575 - val_loss: 97.3978 - val_mse: 97.3978\n",
            "Epoch 233/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 224.2962 - mse: 224.2962 - val_loss: 135.2889 - val_mse: 135.2889\n",
            "Epoch 234/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 212.3212 - mse: 212.3212 - val_loss: 109.1295 - val_mse: 109.1295\n",
            "Epoch 235/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 226.4265 - mse: 226.4265 - val_loss: 113.2448 - val_mse: 113.2448\n",
            "Epoch 236/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 232.8015 - mse: 232.8015 - val_loss: 96.9257 - val_mse: 96.9257\n",
            "Epoch 237/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 220.7765 - mse: 220.7765 - val_loss: 104.9430 - val_mse: 104.9430\n",
            "Epoch 238/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 193.0306 - mse: 193.0306 - val_loss: 88.8746 - val_mse: 88.8746\n",
            "Epoch 239/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 195.3663 - mse: 195.3663 - val_loss: 93.4049 - val_mse: 93.4049\n",
            "Epoch 240/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 226.3634 - mse: 226.3634 - val_loss: 124.3664 - val_mse: 124.3664\n",
            "Epoch 241/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 206.0278 - mse: 206.0278 - val_loss: 110.9576 - val_mse: 110.9576\n",
            "Epoch 242/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 219.1994 - mse: 219.1994 - val_loss: 122.3005 - val_mse: 122.3005\n",
            "Epoch 243/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 259.9046 - mse: 259.9046 - val_loss: 245.3027 - val_mse: 245.3027\n",
            "Epoch 244/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 333.3329 - mse: 333.3329 - val_loss: 161.8823 - val_mse: 161.8823\n",
            "Epoch 245/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 212.4634 - mse: 212.4634 - val_loss: 103.7359 - val_mse: 103.7359\n",
            "Epoch 246/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 205.1186 - mse: 205.1186 - val_loss: 87.7346 - val_mse: 87.7346\n",
            "Epoch 247/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 225.2353 - mse: 225.2353 - val_loss: 187.5336 - val_mse: 187.5336\n",
            "Epoch 248/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 267.4878 - mse: 267.4878 - val_loss: 187.9267 - val_mse: 187.9267\n",
            "Epoch 249/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 227.8191 - mse: 227.8191 - val_loss: 103.5689 - val_mse: 103.5689\n",
            "Epoch 250/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 266.2705 - mse: 266.2705 - val_loss: 124.1581 - val_mse: 124.1581\n",
            "Epoch 251/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 210.3834 - mse: 210.3834 - val_loss: 99.5713 - val_mse: 99.5713\n",
            "Epoch 252/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 187.4400 - mse: 187.4400 - val_loss: 90.6120 - val_mse: 90.6120\n",
            "Epoch 253/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 186.2609 - mse: 186.2609 - val_loss: 88.4391 - val_mse: 88.4391\n",
            "Epoch 254/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 196.9399 - mse: 196.9399 - val_loss: 77.9831 - val_mse: 77.9831\n",
            "Epoch 255/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 196.3835 - mse: 196.3835 - val_loss: 84.9274 - val_mse: 84.9274\n",
            "Epoch 256/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 277.5718 - mse: 277.5718 - val_loss: 134.7198 - val_mse: 134.7198\n",
            "Epoch 257/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 272.3509 - mse: 272.3509 - val_loss: 220.7685 - val_mse: 220.7685\n",
            "Epoch 258/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 241.0847 - mse: 241.0847 - val_loss: 85.1849 - val_mse: 85.1849\n",
            "Epoch 259/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 189.8740 - mse: 189.8740 - val_loss: 250.9047 - val_mse: 250.9047\n",
            "Epoch 260/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 219.3034 - mse: 219.3034 - val_loss: 115.6892 - val_mse: 115.6892\n",
            "Epoch 261/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 221.7717 - mse: 221.7717 - val_loss: 85.5265 - val_mse: 85.5265\n",
            "Epoch 262/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 222.7598 - mse: 222.7598 - val_loss: 125.7407 - val_mse: 125.7407\n",
            "Epoch 263/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 220.2129 - mse: 220.2129 - val_loss: 107.0816 - val_mse: 107.0816\n",
            "Epoch 264/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 245.4009 - mse: 245.4009 - val_loss: 99.5167 - val_mse: 99.5167\n",
            "Epoch 265/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 273.8762 - mse: 273.8762 - val_loss: 294.6745 - val_mse: 294.6745\n",
            "Epoch 266/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 216.1239 - mse: 216.1239 - val_loss: 196.1300 - val_mse: 196.1300\n",
            "Epoch 267/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 338.3241 - mse: 338.3241 - val_loss: 98.0523 - val_mse: 98.0523\n",
            "Epoch 268/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 297.7788 - mse: 297.7788 - val_loss: 407.2890 - val_mse: 407.2890\n",
            "Epoch 269/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 342.9847 - mse: 342.9847 - val_loss: 273.6039 - val_mse: 273.6039\n",
            "Epoch 270/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 282.4517 - mse: 282.4517 - val_loss: 127.3803 - val_mse: 127.3803\n",
            "Epoch 271/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 257.1708 - mse: 257.1708 - val_loss: 83.8834 - val_mse: 83.8834\n",
            "Epoch 272/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 260.3677 - mse: 260.3677 - val_loss: 134.3268 - val_mse: 134.3268\n",
            "Epoch 273/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 262.1735 - mse: 262.1735 - val_loss: 252.3003 - val_mse: 252.3003\n",
            "Epoch 274/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 263.2019 - mse: 263.2019 - val_loss: 123.3635 - val_mse: 123.3635\n",
            "Epoch 275/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 188.6356 - mse: 188.6356 - val_loss: 92.1861 - val_mse: 92.1861\n",
            "Epoch 276/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 204.4292 - mse: 204.4292 - val_loss: 87.1727 - val_mse: 87.1727\n",
            "Epoch 277/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 207.3351 - mse: 207.3351 - val_loss: 258.6168 - val_mse: 258.6168\n",
            "Epoch 278/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 356.6395 - mse: 356.6395 - val_loss: 143.1635 - val_mse: 143.1635\n",
            "Epoch 279/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 269.7253 - mse: 269.7253 - val_loss: 263.8086 - val_mse: 263.8086\n",
            "Epoch 280/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 221.8040 - mse: 221.8040 - val_loss: 173.9799 - val_mse: 173.9799\n",
            "Epoch 281/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 266.4943 - mse: 266.4943 - val_loss: 144.5482 - val_mse: 144.5482\n",
            "Epoch 282/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 191.7356 - mse: 191.7356 - val_loss: 108.5567 - val_mse: 108.5567\n",
            "Epoch 283/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 225.7978 - mse: 225.7978 - val_loss: 119.9111 - val_mse: 119.9111\n",
            "Epoch 284/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 195.2870 - mse: 195.2870 - val_loss: 83.9707 - val_mse: 83.9707\n",
            "Epoch 285/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 191.4300 - mse: 191.4300 - val_loss: 95.0840 - val_mse: 95.0840\n",
            "Epoch 286/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 184.6392 - mse: 184.6392 - val_loss: 142.2735 - val_mse: 142.2735\n",
            "Epoch 287/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 205.6254 - mse: 205.6254 - val_loss: 103.7184 - val_mse: 103.7184\n",
            "Epoch 288/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 208.0006 - mse: 208.0006 - val_loss: 98.6917 - val_mse: 98.6917\n",
            "Epoch 289/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 175.2754 - mse: 175.2754 - val_loss: 90.5867 - val_mse: 90.5867\n",
            "Epoch 290/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 193.5319 - mse: 193.5319 - val_loss: 87.6732 - val_mse: 87.6732\n",
            "Epoch 291/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 207.1824 - mse: 207.1824 - val_loss: 130.6866 - val_mse: 130.6866\n",
            "Epoch 292/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 229.8488 - mse: 229.8488 - val_loss: 87.4896 - val_mse: 87.4896\n",
            "Epoch 293/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 256.1232 - mse: 256.1232 - val_loss: 95.6400 - val_mse: 95.6400\n",
            "Epoch 294/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 225.8940 - mse: 225.8940 - val_loss: 84.5569 - val_mse: 84.5569\n",
            "Epoch 295/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 258.8491 - mse: 258.8491 - val_loss: 85.6044 - val_mse: 85.6044\n",
            "Epoch 296/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 254.6033 - mse: 254.6033 - val_loss: 113.4247 - val_mse: 113.4247\n",
            "Epoch 297/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 190.3196 - mse: 190.3196 - val_loss: 91.3455 - val_mse: 91.3455\n",
            "Epoch 298/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 210.8333 - mse: 210.8333 - val_loss: 94.7508 - val_mse: 94.7508\n",
            "Epoch 299/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 216.3131 - mse: 216.3131 - val_loss: 149.8583 - val_mse: 149.8583\n",
            "Epoch 300/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 267.8473 - mse: 267.8473 - val_loss: 129.7234 - val_mse: 129.7234\n",
            "Epoch 301/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 225.4963 - mse: 225.4963 - val_loss: 128.1839 - val_mse: 128.1839\n",
            "Epoch 302/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 223.9781 - mse: 223.9781 - val_loss: 92.5104 - val_mse: 92.5104\n",
            "Epoch 303/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 250.8184 - mse: 250.8184 - val_loss: 108.1044 - val_mse: 108.1044\n",
            "Epoch 304/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 252.3282 - mse: 252.3282 - val_loss: 225.9396 - val_mse: 225.9396\n",
            "Epoch 305/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 213.7041 - mse: 213.7041 - val_loss: 105.1318 - val_mse: 105.1318\n",
            "Epoch 306/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 224.2415 - mse: 224.2415 - val_loss: 90.6383 - val_mse: 90.6383\n",
            "Epoch 307/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 185.9118 - mse: 185.9118 - val_loss: 83.4248 - val_mse: 83.4248\n",
            "Epoch 308/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 210.1625 - mse: 210.1625 - val_loss: 97.9076 - val_mse: 97.9076\n",
            "Epoch 309/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 205.6522 - mse: 205.6522 - val_loss: 110.7466 - val_mse: 110.7466\n",
            "Epoch 310/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 209.4613 - mse: 209.4613 - val_loss: 85.0001 - val_mse: 85.0001\n",
            "Epoch 311/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 197.4155 - mse: 197.4155 - val_loss: 98.9334 - val_mse: 98.9334\n",
            "Epoch 312/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 173.5198 - mse: 173.5198 - val_loss: 88.2848 - val_mse: 88.2848\n",
            "Epoch 313/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 171.1580 - mse: 171.1580 - val_loss: 105.5620 - val_mse: 105.5620\n",
            "Epoch 314/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 211.7229 - mse: 211.7229 - val_loss: 126.1191 - val_mse: 126.1191\n",
            "Epoch 315/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 185.4090 - mse: 185.4090 - val_loss: 117.6341 - val_mse: 117.6341\n",
            "Epoch 316/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 186.4842 - mse: 186.4842 - val_loss: 83.6792 - val_mse: 83.6792\n",
            "Epoch 317/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 163.3255 - mse: 163.3255 - val_loss: 98.6827 - val_mse: 98.6827\n",
            "Epoch 318/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 189.1480 - mse: 189.1480 - val_loss: 74.8906 - val_mse: 74.8906\n",
            "Epoch 319/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 169.4302 - mse: 169.4302 - val_loss: 77.9666 - val_mse: 77.9666\n",
            "Epoch 320/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 189.0779 - mse: 189.0779 - val_loss: 86.2775 - val_mse: 86.2775\n",
            "Epoch 321/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 186.8383 - mse: 186.8383 - val_loss: 82.0181 - val_mse: 82.0181\n",
            "Epoch 322/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 173.3130 - mse: 173.3130 - val_loss: 82.1576 - val_mse: 82.1576\n",
            "Epoch 323/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 176.4961 - mse: 176.4961 - val_loss: 85.3323 - val_mse: 85.3323\n",
            "Epoch 324/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 169.3838 - mse: 169.3838 - val_loss: 81.6423 - val_mse: 81.6423\n",
            "Epoch 325/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 175.1959 - mse: 175.1959 - val_loss: 236.3719 - val_mse: 236.3719\n",
            "Epoch 326/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 226.9684 - mse: 226.9684 - val_loss: 84.7572 - val_mse: 84.7572\n",
            "Epoch 327/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 176.1551 - mse: 176.1551 - val_loss: 81.0942 - val_mse: 81.0942\n",
            "Epoch 328/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 182.7895 - mse: 182.7895 - val_loss: 84.7401 - val_mse: 84.7401\n",
            "Epoch 329/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 190.3599 - mse: 190.3599 - val_loss: 197.0018 - val_mse: 197.0018\n",
            "Epoch 330/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 259.9396 - mse: 259.9396 - val_loss: 90.9915 - val_mse: 90.9915\n",
            "Epoch 331/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 190.8145 - mse: 190.8145 - val_loss: 153.8979 - val_mse: 153.8979\n",
            "Epoch 332/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 219.6246 - mse: 219.6246 - val_loss: 90.3053 - val_mse: 90.3053\n",
            "Epoch 333/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 174.2645 - mse: 174.2645 - val_loss: 239.2598 - val_mse: 239.2598\n",
            "Epoch 334/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 302.5184 - mse: 302.5184 - val_loss: 163.6212 - val_mse: 163.6212\n",
            "Epoch 335/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 253.3578 - mse: 253.3578 - val_loss: 92.1336 - val_mse: 92.1336\n",
            "Epoch 336/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 174.2501 - mse: 174.2501 - val_loss: 83.5947 - val_mse: 83.5947\n",
            "Epoch 337/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 190.6319 - mse: 190.6319 - val_loss: 218.8253 - val_mse: 218.8253\n",
            "Epoch 338/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 217.9788 - mse: 217.9788 - val_loss: 93.8972 - val_mse: 93.8972\n",
            "Epoch 339/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 197.2820 - mse: 197.2820 - val_loss: 137.3151 - val_mse: 137.3151\n",
            "Epoch 340/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 192.1146 - mse: 192.1146 - val_loss: 139.2634 - val_mse: 139.2634\n",
            "Epoch 341/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 227.1862 - mse: 227.1862 - val_loss: 115.8589 - val_mse: 115.8589\n",
            "Epoch 342/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 186.7572 - mse: 186.7572 - val_loss: 85.9539 - val_mse: 85.9539\n",
            "Epoch 343/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 172.9735 - mse: 172.9735 - val_loss: 162.4196 - val_mse: 162.4196\n",
            "Epoch 344/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 192.1805 - mse: 192.1805 - val_loss: 148.7827 - val_mse: 148.7827\n",
            "Epoch 345/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 247.2326 - mse: 247.2326 - val_loss: 104.7692 - val_mse: 104.7692\n",
            "Epoch 346/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 206.3601 - mse: 206.3601 - val_loss: 84.8408 - val_mse: 84.8408\n",
            "Epoch 347/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 177.0645 - mse: 177.0645 - val_loss: 106.3408 - val_mse: 106.3408\n",
            "Epoch 348/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 189.0749 - mse: 189.0749 - val_loss: 86.4803 - val_mse: 86.4803\n",
            "Epoch 349/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 190.2749 - mse: 190.2749 - val_loss: 80.3551 - val_mse: 80.3551\n",
            "Epoch 350/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 210.5421 - mse: 210.5421 - val_loss: 114.7161 - val_mse: 114.7161\n",
            "Epoch 351/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 198.0100 - mse: 198.0100 - val_loss: 205.3733 - val_mse: 205.3733\n",
            "Epoch 352/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 271.2901 - mse: 271.2901 - val_loss: 81.7581 - val_mse: 81.7581\n",
            "Epoch 353/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 200.6989 - mse: 200.6989 - val_loss: 130.6514 - val_mse: 130.6514\n",
            "Epoch 354/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 195.7967 - mse: 195.7967 - val_loss: 164.0756 - val_mse: 164.0756\n",
            "Epoch 355/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 191.9247 - mse: 191.9247 - val_loss: 139.1456 - val_mse: 139.1456\n",
            "Epoch 356/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 256.6005 - mse: 256.6005 - val_loss: 93.5303 - val_mse: 93.5303\n",
            "Epoch 357/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 220.0116 - mse: 220.0116 - val_loss: 135.7558 - val_mse: 135.7558\n",
            "Epoch 358/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 223.4717 - mse: 223.4717 - val_loss: 103.6232 - val_mse: 103.6232\n",
            "Epoch 359/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 170.9974 - mse: 170.9974 - val_loss: 80.6382 - val_mse: 80.6382\n",
            "Epoch 360/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 163.5117 - mse: 163.5117 - val_loss: 77.8353 - val_mse: 77.8353\n",
            "Epoch 361/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 181.7077 - mse: 181.7077 - val_loss: 133.4137 - val_mse: 133.4137\n",
            "Epoch 362/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 240.5787 - mse: 240.5787 - val_loss: 82.7329 - val_mse: 82.7329\n",
            "Epoch 363/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 209.7259 - mse: 209.7259 - val_loss: 86.8228 - val_mse: 86.8228\n",
            "Epoch 364/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 168.7222 - mse: 168.7222 - val_loss: 239.7458 - val_mse: 239.7458\n",
            "Epoch 365/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 286.4349 - mse: 286.4349 - val_loss: 213.4568 - val_mse: 213.4568\n",
            "Epoch 366/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 173.2435 - mse: 173.2435 - val_loss: 135.0197 - val_mse: 135.0197\n",
            "Epoch 367/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 168.0084 - mse: 168.0084 - val_loss: 78.9591 - val_mse: 78.9591\n",
            "Epoch 368/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 181.3163 - mse: 181.3163 - val_loss: 85.6417 - val_mse: 85.6417\n",
            "Epoch 369/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 200.0619 - mse: 200.0619 - val_loss: 82.6057 - val_mse: 82.6057\n",
            "Epoch 370/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 206.7347 - mse: 206.7347 - val_loss: 198.9586 - val_mse: 198.9586\n",
            "Epoch 371/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 220.0262 - mse: 220.0262 - val_loss: 84.0669 - val_mse: 84.0669\n",
            "Epoch 372/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 190.7026 - mse: 190.7026 - val_loss: 174.3280 - val_mse: 174.3280\n",
            "Epoch 373/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 241.3054 - mse: 241.3054 - val_loss: 83.7350 - val_mse: 83.7350\n",
            "Epoch 374/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 163.0403 - mse: 163.0403 - val_loss: 75.1851 - val_mse: 75.1851\n",
            "Epoch 375/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 159.2716 - mse: 159.2716 - val_loss: 86.3436 - val_mse: 86.3436\n",
            "Epoch 376/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 164.3584 - mse: 164.3584 - val_loss: 84.6224 - val_mse: 84.6224\n",
            "Epoch 377/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 190.5236 - mse: 190.5236 - val_loss: 98.2637 - val_mse: 98.2637\n",
            "Epoch 378/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 163.8111 - mse: 163.8111 - val_loss: 97.3570 - val_mse: 97.3570\n",
            "Epoch 379/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 186.6328 - mse: 186.6328 - val_loss: 260.4595 - val_mse: 260.4595\n",
            "Epoch 380/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 257.8003 - mse: 257.8003 - val_loss: 80.3061 - val_mse: 80.3061\n",
            "Epoch 381/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 166.8019 - mse: 166.8019 - val_loss: 132.1904 - val_mse: 132.1904\n",
            "Epoch 382/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 210.2260 - mse: 210.2260 - val_loss: 153.3841 - val_mse: 153.3841\n",
            "Epoch 383/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 185.8010 - mse: 185.8010 - val_loss: 150.5229 - val_mse: 150.5229\n",
            "Epoch 384/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 246.3559 - mse: 246.3559 - val_loss: 110.1875 - val_mse: 110.1875\n",
            "Epoch 385/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 191.5507 - mse: 191.5507 - val_loss: 83.6406 - val_mse: 83.6406\n",
            "Epoch 386/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 201.9162 - mse: 201.9162 - val_loss: 102.8491 - val_mse: 102.8491\n",
            "Epoch 387/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 238.2769 - mse: 238.2769 - val_loss: 109.1985 - val_mse: 109.1985\n",
            "Epoch 388/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 193.2163 - mse: 193.2163 - val_loss: 84.3513 - val_mse: 84.3513\n",
            "Epoch 389/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 170.0627 - mse: 170.0627 - val_loss: 81.7736 - val_mse: 81.7736\n",
            "Epoch 390/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 163.4383 - mse: 163.4383 - val_loss: 163.2399 - val_mse: 163.2399\n",
            "Epoch 391/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 193.4038 - mse: 193.4038 - val_loss: 76.3191 - val_mse: 76.3191\n",
            "Epoch 392/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 184.1851 - mse: 184.1851 - val_loss: 75.0798 - val_mse: 75.0798\n",
            "Epoch 393/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 164.0331 - mse: 164.0331 - val_loss: 74.5315 - val_mse: 74.5315\n",
            "Epoch 394/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 199.3407 - mse: 199.3407 - val_loss: 76.4492 - val_mse: 76.4492\n",
            "Epoch 395/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 200.7609 - mse: 200.7609 - val_loss: 123.1474 - val_mse: 123.1474\n",
            "Epoch 396/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 234.2503 - mse: 234.2503 - val_loss: 100.4126 - val_mse: 100.4126\n",
            "Epoch 397/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 252.9073 - mse: 252.9073 - val_loss: 211.6260 - val_mse: 211.6260\n",
            "Epoch 398/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 260.0802 - mse: 260.0802 - val_loss: 235.6721 - val_mse: 235.6721\n",
            "Epoch 399/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 341.9515 - mse: 341.9515 - val_loss: 100.8463 - val_mse: 100.8463\n",
            "Epoch 400/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 316.7379 - mse: 316.7379 - val_loss: 111.4377 - val_mse: 111.4377\n",
            "Epoch 401/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 196.4112 - mse: 196.4112 - val_loss: 270.5950 - val_mse: 270.5950\n",
            "Epoch 402/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 248.4335 - mse: 248.4335 - val_loss: 90.6295 - val_mse: 90.6295\n",
            "Epoch 403/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 175.3776 - mse: 175.3776 - val_loss: 137.5330 - val_mse: 137.5330\n",
            "Epoch 404/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 187.4879 - mse: 187.4879 - val_loss: 81.7302 - val_mse: 81.7302\n",
            "Epoch 405/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 195.5859 - mse: 195.5859 - val_loss: 75.7203 - val_mse: 75.7203\n",
            "Epoch 406/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 180.9929 - mse: 180.9929 - val_loss: 85.3102 - val_mse: 85.3102\n",
            "Epoch 407/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 166.0192 - mse: 166.0192 - val_loss: 73.8109 - val_mse: 73.8109\n",
            "Epoch 408/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 152.7340 - mse: 152.7340 - val_loss: 79.7215 - val_mse: 79.7215\n",
            "Epoch 409/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 165.3841 - mse: 165.3841 - val_loss: 111.0433 - val_mse: 111.0433\n",
            "Epoch 410/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 177.0840 - mse: 177.0840 - val_loss: 91.2341 - val_mse: 91.2341\n",
            "Epoch 411/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 160.9568 - mse: 160.9568 - val_loss: 89.8621 - val_mse: 89.8621\n",
            "Epoch 412/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 185.2478 - mse: 185.2478 - val_loss: 78.9844 - val_mse: 78.9844\n",
            "Epoch 413/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 169.7793 - mse: 169.7793 - val_loss: 67.9182 - val_mse: 67.9182\n",
            "Epoch 414/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 154.6527 - mse: 154.6527 - val_loss: 67.4451 - val_mse: 67.4451\n",
            "Epoch 415/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 194.7321 - mse: 194.7321 - val_loss: 90.2474 - val_mse: 90.2474\n",
            "Epoch 416/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 152.7162 - mse: 152.7162 - val_loss: 75.5758 - val_mse: 75.5758\n",
            "Epoch 417/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 161.1680 - mse: 161.1680 - val_loss: 94.1987 - val_mse: 94.1987\n",
            "Epoch 418/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 181.3250 - mse: 181.3250 - val_loss: 139.8291 - val_mse: 139.8291\n",
            "Epoch 419/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 163.4989 - mse: 163.4989 - val_loss: 95.5574 - val_mse: 95.5574\n",
            "Epoch 420/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 157.2144 - mse: 157.2144 - val_loss: 94.9003 - val_mse: 94.9003\n",
            "Epoch 421/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 185.0488 - mse: 185.0488 - val_loss: 84.0079 - val_mse: 84.0079\n",
            "Epoch 422/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 178.8775 - mse: 178.8775 - val_loss: 90.6156 - val_mse: 90.6156\n",
            "Epoch 423/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 171.6035 - mse: 171.6035 - val_loss: 78.7225 - val_mse: 78.7225\n",
            "Epoch 424/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 178.0524 - mse: 178.0524 - val_loss: 135.2593 - val_mse: 135.2593\n",
            "Epoch 425/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 150.8788 - mse: 150.8788 - val_loss: 80.8936 - val_mse: 80.8936\n",
            "Epoch 426/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 162.3554 - mse: 162.3554 - val_loss: 101.1892 - val_mse: 101.1892\n",
            "Epoch 427/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 176.2232 - mse: 176.2232 - val_loss: 143.7487 - val_mse: 143.7487\n",
            "Epoch 428/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 219.6845 - mse: 219.6845 - val_loss: 65.2515 - val_mse: 65.2515\n",
            "Epoch 429/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 245.4088 - mse: 245.4088 - val_loss: 223.9026 - val_mse: 223.9026\n",
            "Epoch 430/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 208.1349 - mse: 208.1349 - val_loss: 82.4554 - val_mse: 82.4554\n",
            "Epoch 431/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 150.5191 - mse: 150.5191 - val_loss: 79.9054 - val_mse: 79.9054\n",
            "Epoch 432/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 200.1956 - mse: 200.1956 - val_loss: 110.6036 - val_mse: 110.6036\n",
            "Epoch 433/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 157.0632 - mse: 157.0632 - val_loss: 153.3226 - val_mse: 153.3226\n",
            "Epoch 434/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 250.1984 - mse: 250.1984 - val_loss: 84.4181 - val_mse: 84.4181\n",
            "Epoch 435/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 223.2356 - mse: 223.2356 - val_loss: 88.8585 - val_mse: 88.8585\n",
            "Epoch 436/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 159.3960 - mse: 159.3960 - val_loss: 250.4026 - val_mse: 250.4026\n",
            "Epoch 437/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 327.6897 - mse: 327.6897 - val_loss: 188.9676 - val_mse: 188.9676\n",
            "Epoch 438/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 253.1615 - mse: 253.1615 - val_loss: 102.7208 - val_mse: 102.7208\n",
            "Epoch 439/700\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 198.9046 - mse: 198.9046 - val_loss: 100.0821 - val_mse: 100.0821\n",
            "Epoch 440/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 208.0669 - mse: 208.0669 - val_loss: 95.6326 - val_mse: 95.6326\n",
            "Epoch 441/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 150.3980 - mse: 150.3980 - val_loss: 107.1569 - val_mse: 107.1569\n",
            "Epoch 442/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 163.5110 - mse: 163.5110 - val_loss: 241.2578 - val_mse: 241.2578\n",
            "Epoch 443/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 227.9794 - mse: 227.9794 - val_loss: 133.6985 - val_mse: 133.6985\n",
            "Epoch 444/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 221.0602 - mse: 221.0602 - val_loss: 110.2146 - val_mse: 110.2146\n",
            "Epoch 445/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 187.5709 - mse: 187.5709 - val_loss: 287.3159 - val_mse: 287.3159\n",
            "Epoch 446/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 408.9306 - mse: 408.9306 - val_loss: 411.1721 - val_mse: 411.1721\n",
            "Epoch 447/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 294.9762 - mse: 294.9762 - val_loss: 158.6712 - val_mse: 158.6712\n",
            "Epoch 448/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 176.6961 - mse: 176.6961 - val_loss: 111.6860 - val_mse: 111.6860\n",
            "Epoch 449/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 178.1385 - mse: 178.1385 - val_loss: 98.4284 - val_mse: 98.4284\n",
            "Epoch 450/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 241.4175 - mse: 241.4175 - val_loss: 131.2587 - val_mse: 131.2587\n",
            "Epoch 451/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 205.0882 - mse: 205.0882 - val_loss: 143.2934 - val_mse: 143.2934\n",
            "Epoch 452/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 209.4944 - mse: 209.4944 - val_loss: 120.4332 - val_mse: 120.4332\n",
            "Epoch 453/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 204.1453 - mse: 204.1453 - val_loss: 97.5912 - val_mse: 97.5912\n",
            "Epoch 454/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 216.9781 - mse: 216.9781 - val_loss: 173.6762 - val_mse: 173.6762\n",
            "Epoch 455/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 202.5163 - mse: 202.5163 - val_loss: 285.5340 - val_mse: 285.5340\n",
            "Epoch 456/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 228.5065 - mse: 228.5065 - val_loss: 81.6411 - val_mse: 81.6411\n",
            "Epoch 457/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 167.8302 - mse: 167.8302 - val_loss: 93.1208 - val_mse: 93.1208\n",
            "Epoch 458/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 159.3777 - mse: 159.3777 - val_loss: 94.5677 - val_mse: 94.5677\n",
            "Epoch 459/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 159.6696 - mse: 159.6696 - val_loss: 73.9941 - val_mse: 73.9941\n",
            "Epoch 460/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 171.9998 - mse: 171.9998 - val_loss: 72.8291 - val_mse: 72.8291\n",
            "Epoch 461/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 167.2168 - mse: 167.2168 - val_loss: 104.9704 - val_mse: 104.9704\n",
            "Epoch 462/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 199.6529 - mse: 199.6529 - val_loss: 127.7565 - val_mse: 127.7565\n",
            "Epoch 463/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 173.4023 - mse: 173.4023 - val_loss: 79.0496 - val_mse: 79.0496\n",
            "Epoch 464/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 152.6546 - mse: 152.6546 - val_loss: 82.7375 - val_mse: 82.7375\n",
            "Epoch 465/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 184.4914 - mse: 184.4914 - val_loss: 72.6312 - val_mse: 72.6312\n",
            "Epoch 466/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 155.4020 - mse: 155.4020 - val_loss: 69.2588 - val_mse: 69.2588\n",
            "Epoch 467/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 160.8208 - mse: 160.8208 - val_loss: 71.4385 - val_mse: 71.4385\n",
            "Epoch 468/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 168.8611 - mse: 168.8611 - val_loss: 110.6187 - val_mse: 110.6187\n",
            "Epoch 469/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 192.1001 - mse: 192.1001 - val_loss: 93.1346 - val_mse: 93.1346\n",
            "Epoch 470/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 183.7477 - mse: 183.7477 - val_loss: 65.4922 - val_mse: 65.4922\n",
            "Epoch 471/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 215.3421 - mse: 215.3421 - val_loss: 137.3378 - val_mse: 137.3378\n",
            "Epoch 472/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 185.8497 - mse: 185.8497 - val_loss: 114.0304 - val_mse: 114.0304\n",
            "Epoch 473/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 181.6189 - mse: 181.6189 - val_loss: 125.5704 - val_mse: 125.5704\n",
            "Epoch 474/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 155.5369 - mse: 155.5369 - val_loss: 83.0003 - val_mse: 83.0003\n",
            "Epoch 475/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 156.2916 - mse: 156.2916 - val_loss: 73.2785 - val_mse: 73.2785\n",
            "Epoch 476/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 189.2799 - mse: 189.2799 - val_loss: 74.7523 - val_mse: 74.7523\n",
            "Epoch 477/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 150.7358 - mse: 150.7358 - val_loss: 82.6915 - val_mse: 82.6915\n",
            "Epoch 478/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 148.2733 - mse: 148.2733 - val_loss: 66.4337 - val_mse: 66.4337\n",
            "Epoch 479/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 158.6216 - mse: 158.6216 - val_loss: 99.3081 - val_mse: 99.3081\n",
            "Epoch 480/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 165.5362 - mse: 165.5362 - val_loss: 80.4372 - val_mse: 80.4372\n",
            "Epoch 481/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 148.4552 - mse: 148.4552 - val_loss: 77.9954 - val_mse: 77.9954\n",
            "Epoch 482/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 159.0354 - mse: 159.0354 - val_loss: 114.1597 - val_mse: 114.1597\n",
            "Epoch 483/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 177.0459 - mse: 177.0459 - val_loss: 75.0079 - val_mse: 75.0079\n",
            "Epoch 484/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 153.9982 - mse: 153.9982 - val_loss: 83.1350 - val_mse: 83.1350\n",
            "Epoch 485/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 142.2091 - mse: 142.2091 - val_loss: 102.8510 - val_mse: 102.8510\n",
            "Epoch 486/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 143.1564 - mse: 143.1564 - val_loss: 95.8998 - val_mse: 95.8998\n",
            "Epoch 487/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 152.8293 - mse: 152.8293 - val_loss: 69.4917 - val_mse: 69.4917\n",
            "Epoch 488/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 148.1152 - mse: 148.1152 - val_loss: 69.1322 - val_mse: 69.1322\n",
            "Epoch 489/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 195.9197 - mse: 195.9197 - val_loss: 127.8040 - val_mse: 127.8040\n",
            "Epoch 490/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 177.3663 - mse: 177.3663 - val_loss: 84.9170 - val_mse: 84.9170\n",
            "Epoch 491/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 187.1511 - mse: 187.1511 - val_loss: 196.8197 - val_mse: 196.8197\n",
            "Epoch 492/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 280.8792 - mse: 280.8792 - val_loss: 113.1919 - val_mse: 113.1919\n",
            "Epoch 493/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 190.7241 - mse: 190.7241 - val_loss: 99.8615 - val_mse: 99.8616\n",
            "Epoch 494/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 186.5538 - mse: 186.5538 - val_loss: 81.5461 - val_mse: 81.5461\n",
            "Epoch 495/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 227.9842 - mse: 227.9842 - val_loss: 73.9789 - val_mse: 73.9789\n",
            "Epoch 496/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 165.3685 - mse: 165.3685 - val_loss: 92.9357 - val_mse: 92.9357\n",
            "Epoch 497/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 163.8234 - mse: 163.8234 - val_loss: 80.2276 - val_mse: 80.2276\n",
            "Epoch 498/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 145.0766 - mse: 145.0766 - val_loss: 68.3963 - val_mse: 68.3963\n",
            "Epoch 499/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 147.9026 - mse: 147.9026 - val_loss: 104.2986 - val_mse: 104.2986\n",
            "Epoch 500/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 151.5516 - mse: 151.5516 - val_loss: 71.4769 - val_mse: 71.4769\n",
            "Epoch 501/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 187.6954 - mse: 187.6954 - val_loss: 227.1174 - val_mse: 227.1174\n",
            "Epoch 502/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 174.6111 - mse: 174.6111 - val_loss: 77.1725 - val_mse: 77.1725\n",
            "Epoch 503/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 140.8021 - mse: 140.8021 - val_loss: 154.9056 - val_mse: 154.9056\n",
            "Epoch 504/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 162.5738 - mse: 162.5738 - val_loss: 69.4773 - val_mse: 69.4773\n",
            "Epoch 505/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 149.7563 - mse: 149.7563 - val_loss: 71.4482 - val_mse: 71.4482\n",
            "Epoch 506/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 147.4493 - mse: 147.4493 - val_loss: 100.3335 - val_mse: 100.3335\n",
            "Epoch 507/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 174.5341 - mse: 174.5341 - val_loss: 200.4576 - val_mse: 200.4576\n",
            "Epoch 508/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 174.1757 - mse: 174.1757 - val_loss: 99.5964 - val_mse: 99.5964\n",
            "Epoch 509/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 182.3885 - mse: 182.3885 - val_loss: 91.3895 - val_mse: 91.3895\n",
            "Epoch 510/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 176.2768 - mse: 176.2768 - val_loss: 106.4837 - val_mse: 106.4837\n",
            "Epoch 511/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 146.1807 - mse: 146.1807 - val_loss: 141.5207 - val_mse: 141.5207\n",
            "Epoch 512/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 173.8532 - mse: 173.8532 - val_loss: 77.7232 - val_mse: 77.7232\n",
            "Epoch 513/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 149.0770 - mse: 149.0770 - val_loss: 66.2908 - val_mse: 66.2908\n",
            "Epoch 514/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 146.2231 - mse: 146.2231 - val_loss: 73.0855 - val_mse: 73.0855\n",
            "Epoch 515/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 127.5494 - mse: 127.5494 - val_loss: 80.9216 - val_mse: 80.9216\n",
            "Epoch 516/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 167.4275 - mse: 167.4275 - val_loss: 101.1174 - val_mse: 101.1174\n",
            "Epoch 517/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 155.0042 - mse: 155.0042 - val_loss: 83.4858 - val_mse: 83.4858\n",
            "Epoch 518/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 166.1338 - mse: 166.1338 - val_loss: 135.9895 - val_mse: 135.9895\n",
            "Epoch 519/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 171.3116 - mse: 171.3116 - val_loss: 72.8590 - val_mse: 72.8590\n",
            "Epoch 520/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 136.9856 - mse: 136.9856 - val_loss: 77.6309 - val_mse: 77.6309\n",
            "Epoch 521/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 193.4194 - mse: 193.4194 - val_loss: 133.9066 - val_mse: 133.9066\n",
            "Epoch 522/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 192.4471 - mse: 192.4471 - val_loss: 77.1550 - val_mse: 77.1550\n",
            "Epoch 523/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 150.6322 - mse: 150.6322 - val_loss: 106.2919 - val_mse: 106.2919\n",
            "Epoch 524/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 206.3976 - mse: 206.3976 - val_loss: 404.1813 - val_mse: 404.1813\n",
            "Epoch 525/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 348.4453 - mse: 348.4453 - val_loss: 100.5457 - val_mse: 100.5457\n",
            "Epoch 526/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 295.3661 - mse: 295.3661 - val_loss: 168.0117 - val_mse: 168.0117\n",
            "Epoch 527/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 220.8841 - mse: 220.8841 - val_loss: 136.5328 - val_mse: 136.5328\n",
            "Epoch 528/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 168.5072 - mse: 168.5072 - val_loss: 79.2264 - val_mse: 79.2264\n",
            "Epoch 529/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 144.8336 - mse: 144.8336 - val_loss: 78.0852 - val_mse: 78.0852\n",
            "Epoch 530/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 177.7783 - mse: 177.7783 - val_loss: 69.7539 - val_mse: 69.7539\n",
            "Epoch 531/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 149.1101 - mse: 149.1101 - val_loss: 70.2580 - val_mse: 70.2580\n",
            "Epoch 532/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 143.2721 - mse: 143.2721 - val_loss: 107.4357 - val_mse: 107.4357\n",
            "Epoch 533/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 186.5985 - mse: 186.5985 - val_loss: 74.6034 - val_mse: 74.6034\n",
            "Epoch 534/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 157.3795 - mse: 157.3795 - val_loss: 168.8608 - val_mse: 168.8608\n",
            "Epoch 535/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 150.8076 - mse: 150.8076 - val_loss: 71.0669 - val_mse: 71.0669\n",
            "Epoch 536/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 131.4487 - mse: 131.4487 - val_loss: 82.8067 - val_mse: 82.8067\n",
            "Epoch 537/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 140.6559 - mse: 140.6559 - val_loss: 122.4817 - val_mse: 122.4817\n",
            "Epoch 538/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 169.3250 - mse: 169.3250 - val_loss: 85.4371 - val_mse: 85.4371\n",
            "Epoch 539/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 154.7440 - mse: 154.7440 - val_loss: 72.6807 - val_mse: 72.6807\n",
            "Epoch 540/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 141.4358 - mse: 141.4358 - val_loss: 103.8966 - val_mse: 103.8966\n",
            "Epoch 541/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 147.7701 - mse: 147.7701 - val_loss: 72.1949 - val_mse: 72.1949\n",
            "Epoch 542/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 184.9038 - mse: 184.9038 - val_loss: 98.5792 - val_mse: 98.5792\n",
            "Epoch 543/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 313.8304 - mse: 313.8304 - val_loss: 171.0946 - val_mse: 171.0946\n",
            "Epoch 544/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 283.4367 - mse: 283.4367 - val_loss: 92.3337 - val_mse: 92.3338\n",
            "Epoch 545/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 189.0344 - mse: 189.0344 - val_loss: 110.3629 - val_mse: 110.3629\n",
            "Epoch 546/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 145.0535 - mse: 145.0535 - val_loss: 128.7999 - val_mse: 128.7999\n",
            "Epoch 547/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 183.9036 - mse: 183.9036 - val_loss: 130.8091 - val_mse: 130.8091\n",
            "Epoch 548/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 169.2218 - mse: 169.2218 - val_loss: 79.8583 - val_mse: 79.8583\n",
            "Epoch 549/700\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 159.5850 - mse: 159.5850 - val_loss: 137.1605 - val_mse: 137.1605\n",
            "Epoch 550/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 152.7874 - mse: 152.7874 - val_loss: 72.9088 - val_mse: 72.9088\n",
            "Epoch 551/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 151.7174 - mse: 151.7174 - val_loss: 93.7799 - val_mse: 93.7799\n",
            "Epoch 552/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 139.9382 - mse: 139.9382 - val_loss: 73.4527 - val_mse: 73.4527\n",
            "Epoch 553/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 135.1982 - mse: 135.1982 - val_loss: 69.5167 - val_mse: 69.5167\n",
            "Epoch 554/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 143.2432 - mse: 143.2432 - val_loss: 100.1127 - val_mse: 100.1127\n",
            "Epoch 555/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 135.0922 - mse: 135.0922 - val_loss: 78.3243 - val_mse: 78.3243\n",
            "Epoch 556/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 161.8235 - mse: 161.8235 - val_loss: 67.7316 - val_mse: 67.7316\n",
            "Epoch 557/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 158.0827 - mse: 158.0827 - val_loss: 225.4970 - val_mse: 225.4970\n",
            "Epoch 558/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 184.8827 - mse: 184.8827 - val_loss: 81.3153 - val_mse: 81.3153\n",
            "Epoch 559/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 146.6214 - mse: 146.6214 - val_loss: 80.5989 - val_mse: 80.5989\n",
            "Epoch 560/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 190.5860 - mse: 190.5860 - val_loss: 137.5839 - val_mse: 137.5839\n",
            "Epoch 561/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 169.2325 - mse: 169.2325 - val_loss: 73.4411 - val_mse: 73.4411\n",
            "Epoch 562/700\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 147.0135 - mse: 147.0135 - val_loss: 77.5600 - val_mse: 77.5600\n",
            "Epoch 563/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 193.0234 - mse: 193.0234 - val_loss: 214.3501 - val_mse: 214.3501\n",
            "Epoch 564/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 208.6948 - mse: 208.6948 - val_loss: 97.6927 - val_mse: 97.6927\n",
            "Epoch 565/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 247.8369 - mse: 247.8369 - val_loss: 100.2563 - val_mse: 100.2563\n",
            "Epoch 566/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 167.2361 - mse: 167.2361 - val_loss: 80.7246 - val_mse: 80.7246\n",
            "Epoch 567/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 142.4437 - mse: 142.4437 - val_loss: 69.0426 - val_mse: 69.0426\n",
            "Epoch 568/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 128.5809 - mse: 128.5809 - val_loss: 85.5831 - val_mse: 85.5831\n",
            "Epoch 569/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 129.4949 - mse: 129.4949 - val_loss: 73.2444 - val_mse: 73.2444\n",
            "Epoch 570/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 150.2072 - mse: 150.2072 - val_loss: 251.2546 - val_mse: 251.2546\n",
            "Epoch 571/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 264.8439 - mse: 264.8439 - val_loss: 130.9729 - val_mse: 130.9729\n",
            "Epoch 572/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 240.4524 - mse: 240.4524 - val_loss: 221.1894 - val_mse: 221.1894\n",
            "Epoch 573/700\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 338.6309 - mse: 338.6309 - val_loss: 258.5182 - val_mse: 258.5182\n",
            "Epoch 574/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 147.4171 - mse: 147.4171 - val_loss: 89.9866 - val_mse: 89.9866\n",
            "Epoch 575/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 135.5726 - mse: 135.5726 - val_loss: 79.0271 - val_mse: 79.0271\n",
            "Epoch 576/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 228.4834 - mse: 228.4834 - val_loss: 244.7538 - val_mse: 244.7538\n",
            "Epoch 577/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 225.1935 - mse: 225.1935 - val_loss: 115.4278 - val_mse: 115.4278\n",
            "Epoch 578/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 147.0537 - mse: 147.0537 - val_loss: 77.5787 - val_mse: 77.5787\n",
            "Epoch 579/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 145.0831 - mse: 145.0831 - val_loss: 225.8894 - val_mse: 225.8894\n",
            "Epoch 580/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 228.4091 - mse: 228.4091 - val_loss: 80.5920 - val_mse: 80.5920\n",
            "Epoch 581/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 132.0933 - mse: 132.0933 - val_loss: 82.3539 - val_mse: 82.3539\n",
            "Epoch 582/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 167.3812 - mse: 167.3812 - val_loss: 94.0918 - val_mse: 94.0918\n",
            "Epoch 583/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 164.3429 - mse: 164.3429 - val_loss: 74.3991 - val_mse: 74.3991\n",
            "Epoch 584/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 130.5003 - mse: 130.5003 - val_loss: 72.6866 - val_mse: 72.6866\n",
            "Epoch 585/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 172.3222 - mse: 172.3222 - val_loss: 83.5152 - val_mse: 83.5152\n",
            "Epoch 586/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 160.7159 - mse: 160.7159 - val_loss: 102.9394 - val_mse: 102.9394\n",
            "Epoch 587/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 173.5271 - mse: 173.5271 - val_loss: 115.3234 - val_mse: 115.3234\n",
            "Epoch 588/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 183.9628 - mse: 183.9628 - val_loss: 100.0715 - val_mse: 100.0715\n",
            "Epoch 589/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 194.1996 - mse: 194.1996 - val_loss: 88.2338 - val_mse: 88.2338\n",
            "Epoch 590/700\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 155.4777 - mse: 155.4777 - val_loss: 73.0233 - val_mse: 73.0233\n",
            "Epoch 591/700\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 129.5924 - mse: 129.5924 - val_loss: 100.7047 - val_mse: 100.7047\n",
            "Epoch 592/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 153.3711 - mse: 153.3711 - val_loss: 71.1091 - val_mse: 71.1091\n",
            "Epoch 593/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 155.3044 - mse: 155.3044 - val_loss: 130.8196 - val_mse: 130.8196\n",
            "Epoch 594/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 161.4186 - mse: 161.4186 - val_loss: 111.0734 - val_mse: 111.0734\n",
            "Epoch 595/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 146.6569 - mse: 146.6569 - val_loss: 65.7265 - val_mse: 65.7265\n",
            "Epoch 596/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 128.7149 - mse: 128.7149 - val_loss: 66.6036 - val_mse: 66.6036\n",
            "Epoch 597/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 132.6990 - mse: 132.6990 - val_loss: 74.1218 - val_mse: 74.1218\n",
            "Epoch 598/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 129.8094 - mse: 129.8094 - val_loss: 70.8217 - val_mse: 70.8217\n",
            "Epoch 599/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 135.3901 - mse: 135.3901 - val_loss: 112.1100 - val_mse: 112.1100\n",
            "Epoch 600/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 137.5500 - mse: 137.5500 - val_loss: 143.1266 - val_mse: 143.1266\n",
            "Epoch 601/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 147.6863 - mse: 147.6863 - val_loss: 81.5451 - val_mse: 81.5451\n",
            "Epoch 602/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 144.3440 - mse: 144.3440 - val_loss: 88.6528 - val_mse: 88.6528\n",
            "Epoch 603/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 161.4173 - mse: 161.4173 - val_loss: 111.8825 - val_mse: 111.8825\n",
            "Epoch 604/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 150.7964 - mse: 150.7964 - val_loss: 78.2030 - val_mse: 78.2030\n",
            "Epoch 605/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 130.2821 - mse: 130.2821 - val_loss: 85.1293 - val_mse: 85.1293\n",
            "Epoch 606/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 134.4514 - mse: 134.4514 - val_loss: 74.6802 - val_mse: 74.6802\n",
            "Epoch 607/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 136.1772 - mse: 136.1772 - val_loss: 70.0994 - val_mse: 70.0994\n",
            "Epoch 608/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 133.8621 - mse: 133.8621 - val_loss: 114.1539 - val_mse: 114.1539\n",
            "Epoch 609/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 190.3207 - mse: 190.3207 - val_loss: 158.8846 - val_mse: 158.8846\n",
            "Epoch 610/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 287.9844 - mse: 287.9844 - val_loss: 236.8969 - val_mse: 236.8969\n",
            "Epoch 611/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 276.8390 - mse: 276.8390 - val_loss: 162.8996 - val_mse: 162.8996\n",
            "Epoch 612/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 278.6570 - mse: 278.6570 - val_loss: 232.9098 - val_mse: 232.9098\n",
            "Epoch 613/700\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 214.8983 - mse: 214.8983 - val_loss: 116.6306 - val_mse: 116.6306\n",
            "Epoch 614/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 173.8303 - mse: 173.8303 - val_loss: 188.6613 - val_mse: 188.6613\n",
            "Epoch 615/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 198.1468 - mse: 198.1468 - val_loss: 126.9694 - val_mse: 126.9694\n",
            "Epoch 616/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 190.9405 - mse: 190.9405 - val_loss: 105.7202 - val_mse: 105.7202\n",
            "Epoch 617/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 158.3484 - mse: 158.3484 - val_loss: 80.8365 - val_mse: 80.8365\n",
            "Epoch 618/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 128.1556 - mse: 128.1556 - val_loss: 74.1846 - val_mse: 74.1846\n",
            "Epoch 619/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 130.5839 - mse: 130.5839 - val_loss: 136.8462 - val_mse: 136.8462\n",
            "Epoch 620/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 156.9104 - mse: 156.9104 - val_loss: 195.7214 - val_mse: 195.7214\n",
            "Epoch 621/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 162.1171 - mse: 162.1171 - val_loss: 131.9529 - val_mse: 131.9529\n",
            "Epoch 622/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 239.8319 - mse: 239.8319 - val_loss: 84.8831 - val_mse: 84.8831\n",
            "Epoch 623/700\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 151.8278 - mse: 151.8278 - val_loss: 85.5095 - val_mse: 85.5095\n",
            "Epoch 624/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 155.5217 - mse: 155.5217 - val_loss: 78.2728 - val_mse: 78.2728\n",
            "Epoch 625/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 142.0863 - mse: 142.0863 - val_loss: 121.1632 - val_mse: 121.1632\n",
            "Epoch 626/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 211.7858 - mse: 211.7858 - val_loss: 142.6918 - val_mse: 142.6918\n",
            "Epoch 627/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 167.0330 - mse: 167.0330 - val_loss: 85.6202 - val_mse: 85.6202\n",
            "Epoch 628/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 156.1104 - mse: 156.1104 - val_loss: 106.5209 - val_mse: 106.5209\n",
            "Epoch 629/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 150.1645 - mse: 150.1645 - val_loss: 108.6365 - val_mse: 108.6365\n",
            "Epoch 630/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 135.5286 - mse: 135.5286 - val_loss: 140.6424 - val_mse: 140.6424\n",
            "Epoch 631/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 157.8854 - mse: 157.8854 - val_loss: 76.3302 - val_mse: 76.3302\n",
            "Epoch 632/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 121.0005 - mse: 121.0005 - val_loss: 75.4072 - val_mse: 75.4072\n",
            "Epoch 633/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 142.2642 - mse: 142.2642 - val_loss: 147.2220 - val_mse: 147.2220\n",
            "Epoch 634/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 170.0929 - mse: 170.0929 - val_loss: 172.0227 - val_mse: 172.0227\n",
            "Epoch 635/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 198.8994 - mse: 198.8994 - val_loss: 137.1168 - val_mse: 137.1168\n",
            "Epoch 636/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 208.5677 - mse: 208.5677 - val_loss: 91.4146 - val_mse: 91.4146\n",
            "Epoch 637/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 153.5848 - mse: 153.5848 - val_loss: 75.5688 - val_mse: 75.5688\n",
            "Epoch 638/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 139.5257 - mse: 139.5257 - val_loss: 77.6349 - val_mse: 77.6349\n",
            "Epoch 639/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 141.8118 - mse: 141.8118 - val_loss: 73.1313 - val_mse: 73.1313\n",
            "Epoch 640/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 140.8474 - mse: 140.8474 - val_loss: 71.8530 - val_mse: 71.8530\n",
            "Epoch 641/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 117.9436 - mse: 117.9436 - val_loss: 125.9494 - val_mse: 125.9494\n",
            "Epoch 642/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 145.2479 - mse: 145.2479 - val_loss: 127.0913 - val_mse: 127.0913\n",
            "Epoch 643/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 160.1554 - mse: 160.1554 - val_loss: 125.8606 - val_mse: 125.8606\n",
            "Epoch 644/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 139.7276 - mse: 139.7276 - val_loss: 71.7143 - val_mse: 71.7143\n",
            "Epoch 645/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 119.7163 - mse: 119.7163 - val_loss: 95.9316 - val_mse: 95.9316\n",
            "Epoch 646/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 148.8929 - mse: 148.8929 - val_loss: 163.0877 - val_mse: 163.0877\n",
            "Epoch 647/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 153.8143 - mse: 153.8143 - val_loss: 76.5681 - val_mse: 76.5681\n",
            "Epoch 648/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 124.7897 - mse: 124.7897 - val_loss: 83.9688 - val_mse: 83.9688\n",
            "Epoch 649/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 128.0775 - mse: 128.0775 - val_loss: 92.2075 - val_mse: 92.2075\n",
            "Epoch 650/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 147.0330 - mse: 147.0330 - val_loss: 70.3644 - val_mse: 70.3644\n",
            "Epoch 651/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 127.9867 - mse: 127.9867 - val_loss: 88.4024 - val_mse: 88.4024\n",
            "Epoch 652/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 166.1419 - mse: 166.1419 - val_loss: 238.2745 - val_mse: 238.2745\n",
            "Epoch 653/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 183.3660 - mse: 183.3660 - val_loss: 92.0627 - val_mse: 92.0627\n",
            "Epoch 654/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 187.0836 - mse: 187.0836 - val_loss: 83.0904 - val_mse: 83.0904\n",
            "Epoch 655/700\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 148.6441 - mse: 148.6441 - val_loss: 94.4387 - val_mse: 94.4387\n",
            "Epoch 656/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 129.3482 - mse: 129.3482 - val_loss: 108.6899 - val_mse: 108.6899\n",
            "Epoch 657/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 156.3181 - mse: 156.3181 - val_loss: 91.3773 - val_mse: 91.3773\n",
            "Epoch 658/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 131.4120 - mse: 131.4120 - val_loss: 83.7033 - val_mse: 83.7033\n",
            "Epoch 659/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 139.8478 - mse: 139.8478 - val_loss: 73.9741 - val_mse: 73.9741\n",
            "Epoch 660/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 121.3946 - mse: 121.3946 - val_loss: 69.3085 - val_mse: 69.3085\n",
            "Epoch 661/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 142.2753 - mse: 142.2753 - val_loss: 107.4963 - val_mse: 107.4963\n",
            "Epoch 662/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 141.9427 - mse: 141.9427 - val_loss: 79.8504 - val_mse: 79.8504\n",
            "Epoch 663/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 119.8933 - mse: 119.8933 - val_loss: 72.1418 - val_mse: 72.1418\n",
            "Epoch 664/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 128.0547 - mse: 128.0547 - val_loss: 107.3819 - val_mse: 107.3819\n",
            "Epoch 665/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 157.4232 - mse: 157.4232 - val_loss: 144.2342 - val_mse: 144.2342\n",
            "Epoch 666/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 191.1021 - mse: 191.1021 - val_loss: 79.8709 - val_mse: 79.8709\n",
            "Epoch 667/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 263.5557 - mse: 263.5557 - val_loss: 132.8660 - val_mse: 132.8660\n",
            "Epoch 668/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 147.8941 - mse: 147.8941 - val_loss: 163.2513 - val_mse: 163.2513\n",
            "Epoch 669/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 165.9891 - mse: 165.9891 - val_loss: 98.9097 - val_mse: 98.9097\n",
            "Epoch 670/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 150.5294 - mse: 150.5294 - val_loss: 84.9374 - val_mse: 84.9374\n",
            "Epoch 671/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 147.3161 - mse: 147.3161 - val_loss: 106.8835 - val_mse: 106.8835\n",
            "Epoch 672/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 136.3036 - mse: 136.3036 - val_loss: 82.8788 - val_mse: 82.8788\n",
            "Epoch 673/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 126.7073 - mse: 126.7073 - val_loss: 70.3549 - val_mse: 70.3549\n",
            "Epoch 674/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 128.5785 - mse: 128.5785 - val_loss: 138.5765 - val_mse: 138.5765\n",
            "Epoch 675/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 205.6966 - mse: 205.6966 - val_loss: 204.1180 - val_mse: 204.1180\n",
            "Epoch 676/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 190.7996 - mse: 190.7996 - val_loss: 157.9833 - val_mse: 157.9833\n",
            "Epoch 677/700\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 215.4346 - mse: 215.4346 - val_loss: 122.5516 - val_mse: 122.5516\n",
            "Epoch 678/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 254.5142 - mse: 254.5142 - val_loss: 101.6330 - val_mse: 101.6330\n",
            "Epoch 679/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 169.7837 - mse: 169.7837 - val_loss: 111.4235 - val_mse: 111.4235\n",
            "Epoch 680/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 188.7132 - mse: 188.7132 - val_loss: 101.9308 - val_mse: 101.9308\n",
            "Epoch 681/700\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 132.1556 - mse: 132.1556 - val_loss: 85.9645 - val_mse: 85.9645\n",
            "Epoch 682/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 134.7202 - mse: 134.7202 - val_loss: 79.5186 - val_mse: 79.5186\n",
            "Epoch 683/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 117.3085 - mse: 117.3085 - val_loss: 112.5540 - val_mse: 112.5540\n",
            "Epoch 684/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 120.0014 - mse: 120.0014 - val_loss: 78.8565 - val_mse: 78.8565\n",
            "Epoch 685/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 122.8427 - mse: 122.8427 - val_loss: 96.0132 - val_mse: 96.0132\n",
            "Epoch 686/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 143.1373 - mse: 143.1373 - val_loss: 105.3758 - val_mse: 105.3758\n",
            "Epoch 687/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 121.9651 - mse: 121.9651 - val_loss: 74.3784 - val_mse: 74.3784\n",
            "Epoch 688/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 121.7434 - mse: 121.7434 - val_loss: 72.7442 - val_mse: 72.7442\n",
            "Epoch 689/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 150.6589 - mse: 150.6589 - val_loss: 120.6569 - val_mse: 120.6569\n",
            "Epoch 690/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 211.3247 - mse: 211.3247 - val_loss: 109.9898 - val_mse: 109.9898\n",
            "Epoch 691/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 152.2256 - mse: 152.2256 - val_loss: 377.4010 - val_mse: 377.4010\n",
            "Epoch 692/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 338.5167 - mse: 338.5167 - val_loss: 314.0031 - val_mse: 314.0031\n",
            "Epoch 693/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 200.8497 - mse: 200.8497 - val_loss: 94.6937 - val_mse: 94.6937\n",
            "Epoch 694/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 113.2955 - mse: 113.2955 - val_loss: 92.4306 - val_mse: 92.4306\n",
            "Epoch 695/700\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 142.9544 - mse: 142.9544 - val_loss: 96.9061 - val_mse: 96.9061\n",
            "Epoch 696/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 118.3623 - mse: 118.3623 - val_loss: 82.8364 - val_mse: 82.8364\n",
            "Epoch 697/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 121.7856 - mse: 121.7856 - val_loss: 67.5590 - val_mse: 67.5590\n",
            "Epoch 698/700\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 161.3197 - mse: 161.3197 - val_loss: 108.4909 - val_mse: 108.4909\n",
            "Epoch 699/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 261.2304 - mse: 261.2304 - val_loss: 118.6655 - val_mse: 118.6655\n",
            "Epoch 700/700\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 208.7686 - mse: 208.7686 - val_loss: 135.8552 - val_mse: 135.8552\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7abc80e710>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bULw888W4249",
        "outputId": "2ad6c326-aebc-4dbb-ccfd-fddfcc1b929e"
      },
      "source": [
        "res = model.predict(X, verbose= 0)\n",
        "res"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[113.05921 ],\n",
              "       [113.05921 ],\n",
              "       [125.62122 ],\n",
              "       [ 87.33242 ],\n",
              "       [118.50762 ],\n",
              "       [104.052086],\n",
              "       [109.595764],\n",
              "       [114.70277 ],\n",
              "       [125.921776],\n",
              "       [135.31374 ],\n",
              "       [ 97.49517 ],\n",
              "       [ 97.49517 ],\n",
              "       [104.37217 ],\n",
              "       [104.58895 ],\n",
              "       [103.17147 ],\n",
              "       [177.13235 ],\n",
              "       [175.84187 ],\n",
              "       [167.77267 ],\n",
              "       [ 52.805573],\n",
              "       [ 67.117615],\n",
              "       [ 67.97204 ],\n",
              "       [ 68.42614 ],\n",
              "       [ 68.42988 ],\n",
              "       [ 88.95386 ],\n",
              "       [ 71.61612 ],\n",
              "       [ 72.263664],\n",
              "       [ 72.263664],\n",
              "       [ 92.96567 ],\n",
              "       [ 83.419136],\n",
              "       [130.56364 ],\n",
              "       [ 58.856266],\n",
              "       [ 75.23223 ],\n",
              "       [ 65.39444 ],\n",
              "       [ 76.29169 ],\n",
              "       [ 76.86152 ],\n",
              "       [ 76.00556 ],\n",
              "       [ 76.43556 ],\n",
              "       [ 84.635605],\n",
              "       [ 87.640144],\n",
              "       [ 86.49021 ],\n",
              "       [ 91.08156 ],\n",
              "       [ 99.97853 ],\n",
              "       [ 85.81812 ],\n",
              "       [ 78.606186],\n",
              "       [ 67.117615],\n",
              "       [ 68.36415 ],\n",
              "       [ 99.58237 ],\n",
              "       [177.00238 ],\n",
              "       [177.00238 ],\n",
              "       [235.27316 ],\n",
              "       [ 63.40439 ],\n",
              "       [ 63.75994 ],\n",
              "       [ 63.93774 ],\n",
              "       [ 64.24198 ],\n",
              "       [ 64.3887  ],\n",
              "       [ 96.13456 ],\n",
              "       [ 96.13456 ],\n",
              "       [ 96.46321 ],\n",
              "       [105.77655 ],\n",
              "       [ 77.22383 ],\n",
              "       [ 77.38662 ],\n",
              "       [ 77.22383 ],\n",
              "       [ 77.38662 ],\n",
              "       [ 65.60295 ],\n",
              "       [ 77.87407 ],\n",
              "       [105.969284],\n",
              "       [ 74.75674 ],\n",
              "       [115.33401 ],\n",
              "       [124.92496 ],\n",
              "       [117.451805],\n",
              "       [121.10337 ],\n",
              "       [151.96075 ],\n",
              "       [169.59595 ],\n",
              "       [176.58815 ],\n",
              "       [176.20976 ],\n",
              "       [123.17084 ],\n",
              "       [ 70.17595 ],\n",
              "       [ 71.101906],\n",
              "       [ 72.87698 ],\n",
              "       [ 91.38122 ],\n",
              "       [101.8405  ],\n",
              "       [ 82.59229 ],\n",
              "       [136.09317 ],\n",
              "       [142.57556 ],\n",
              "       [142.94388 ],\n",
              "       [ 83.21463 ],\n",
              "       [ 84.84167 ],\n",
              "       [102.14543 ],\n",
              "       [ 96.553055],\n",
              "       [ 64.97081 ],\n",
              "       [ 59.36202 ],\n",
              "       [ 66.00202 ],\n",
              "       [ 66.71316 ],\n",
              "       [ 68.88971 ],\n",
              "       [ 67.0956  ],\n",
              "       [ 69.57055 ],\n",
              "       [ 67.68372 ],\n",
              "       [ 69.27194 ],\n",
              "       [ 69.24938 ],\n",
              "       [ 82.490906],\n",
              "       [ 81.690025],\n",
              "       [151.31808 ],\n",
              "       [150.51208 ],\n",
              "       [147.38959 ],\n",
              "       [166.45074 ],\n",
              "       [176.66379 ],\n",
              "       [160.37129 ],\n",
              "       [105.53531 ],\n",
              "       [ 89.90034 ],\n",
              "       [100.229744],\n",
              "       [ 92.98969 ],\n",
              "       [107.365845],\n",
              "       [ 91.90082 ],\n",
              "       [101.12141 ],\n",
              "       [ 95.38876 ],\n",
              "       [106.92003 ],\n",
              "       [ 91.90082 ],\n",
              "       [134.79494 ],\n",
              "       [ 69.92569 ],\n",
              "       [ 90.096115],\n",
              "       [ 71.61612 ],\n",
              "       [ 71.087135],\n",
              "       [ 79.40345 ],\n",
              "       [ 83.41914 ],\n",
              "       [138.3258  ],\n",
              "       [129.50839 ],\n",
              "       [179.80281 ],\n",
              "       [179.80281 ],\n",
              "       [184.32805 ],\n",
              "       [215.81102 ],\n",
              "       [ 97.474785],\n",
              "       [ 96.09863 ],\n",
              "       [ 96.65436 ],\n",
              "       [ 97.855415],\n",
              "       [ 98.67448 ],\n",
              "       [100.416504],\n",
              "       [119.62528 ],\n",
              "       [121.508316],\n",
              "       [ 71.62847 ],\n",
              "       [ 67.94276 ],\n",
              "       [ 71.24272 ],\n",
              "       [ 70.66344 ],\n",
              "       [ 65.386795],\n",
              "       [ 90.307045],\n",
              "       [ 78.819756],\n",
              "       [ 96.579704],\n",
              "       [ 75.672714],\n",
              "       [ 94.22472 ],\n",
              "       [ 79.36576 ],\n",
              "       [102.26097 ],\n",
              "       [ 63.112007],\n",
              "       [ 63.526375],\n",
              "       [ 63.33803 ],\n",
              "       [ 65.54295 ],\n",
              "       [ 65.336784],\n",
              "       [ 91.41294 ],\n",
              "       [ 64.91607 ],\n",
              "       [ 65.71011 ],\n",
              "       [ 59.738865],\n",
              "       [ 59.81008 ],\n",
              "       [ 65.25105 ],\n",
              "       [ 66.04508 ],\n",
              "       [ 66.464554],\n",
              "       [ 68.61401 ],\n",
              "       [ 69.53281 ],\n",
              "       [100.36134 ],\n",
              "       [102.54085 ],\n",
              "       [101.3424  ],\n",
              "       [101.15669 ],\n",
              "       [101.85313 ],\n",
              "       [107.79584 ],\n",
              "       [109.420784],\n",
              "       [120.81353 ],\n",
              "       [ 67.575455],\n",
              "       [ 65.4237  ],\n",
              "       [ 71.250824],\n",
              "       [ 70.67547 ],\n",
              "       [ 73.30492 ],\n",
              "       [146.16997 ],\n",
              "       [149.11647 ],\n",
              "       [152.02098 ],\n",
              "       [146.4693  ],\n",
              "       [ 58.86901 ],\n",
              "       [ 80.45052 ],\n",
              "       [ 58.94345 ],\n",
              "       [ 80.558395],\n",
              "       [ 82.92421 ],\n",
              "       [ 58.69231 ],\n",
              "       [ 87.48199 ],\n",
              "       [ 90.88332 ],\n",
              "       [ 89.1375  ],\n",
              "       [108.228355],\n",
              "       [ 64.45109 ],\n",
              "       [ 93.91334 ],\n",
              "       [117.32344 ],\n",
              "       [122.23964 ],\n",
              "       [118.39121 ],\n",
              "       [122.611046],\n",
              "       [122.93266 ],\n",
              "       [123.64881 ],\n",
              "       [117.17345 ],\n",
              "       [127.7885  ],\n",
              "       [136.11752 ],\n",
              "       [103.18954 ],\n",
              "       [129.30957 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    }
  ]
}